{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Train CGCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatically search for an NVIDIA GPU and use it. If not, then use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load all of our preprocessed and split data from our cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Label to use for this model in the plots\n",
    "model_name = 'GP-with-CGCNN-mean'\n",
    "\n",
    "with open('../preprocessing/sdt/gasdb/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../preprocessing/splits_gasdb.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "docs_train = splits['docs_train']\n",
    "docs_val = splits['docs_val']\n",
    "sdts_train, sdts_val = splits['sdts_train'], splits['sdts_val']\n",
    "targets_train, targets_val = splits['targets_train'], splits['targets_val']\n",
    "\n",
    "# Where we put the intermediate results for this notebook\n",
    "prefix = 'gasdb_pooled/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the CGCNN `net` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch import callbacks  # needs skorch >= 0.4  \n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import CVSplit\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "# Callback to checkpoint parameters every time there is a new best for validation loss\n",
    "cp = callbacks.Checkpoint(monitor='valid_loss_best', fn_prefix=prefix+'valid_best_')\n",
    "\n",
    "# Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params(prefix+'valid_best_params.pt')\n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "# Callback to set the learning rate dynamically\n",
    "LR_schedule = callbacks.lr_scheduler.LRScheduler('MultiStepLR', milestones=[100], gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "    module__nbr_fea_len=nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=150,\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn=collate_pool,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn=collate_pool,\n",
    "    iterator_valid__shuffle=False,\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the CGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.fit(sdts_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart training in case first round was cut off\n",
    "net.partial_fit(sdts_train, targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ...or load whatever is cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.load_params(f_history=prefix+'valid_best_history.json',\n",
    "                f_optimizer=prefix+'valid_best_optimizer.pt', \n",
    "                f_params=prefix+'valid_best_params.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Train GP-rbf with CGCNN Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "# Get CGCNN predictions on sdts_val, and sdts_train\n",
    "targets_pred = net.predict(sdts_val).reshape(-1)\n",
    "targets_pred_train = net.predict(sdts_train).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fingerprints data\n",
    "fingerprints_train_raw = splits['fingerprints_train']\n",
    "fingerprints_val_raw = splits['fingerprints_val']\n",
    "\n",
    "# Define all the adsorbates\n",
    "adsorbates = list({doc['adsorbate'] for doc in docs_val})\n",
    "adsorbates.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the fingerprints\n",
    "scaler = StandardScaler()\n",
    "fingerprints_train = scaler.fit_transform(fingerprints_train_raw)\n",
    "fingerprints_val = scaler.transform(fingerprints_val_raw)\n",
    "\n",
    "# Make torch.Tensor version of data for GPyTorch\n",
    "fingerprints_train = torch.Tensor(fingerprints_train).contiguous()\n",
    "fingerprints_val = torch.Tensor(fingerprints_val)\n",
    "targets_train_gp = torch.Tensor(targets_train.reshape(-1))\n",
    "targets_val_gp = torch.Tensor(targets_val.reshape(-1))\n",
    "\n",
    "# Transform targets for GP with nonzero mean\n",
    "targets_train_gp = targets_train_gp - torch.Tensor(targets_pred_train)\n",
    "targets_val_gp = targets_val_gp - torch.Tensor(targets_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GP\n",
    "\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(fingerprints_train, targets_train_gp, likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Function to make dynamic learning curve\n",
    "def live_plot(data_dict, figsize=(7, 5), title=''):\n",
    "    ''' Credit to Ziofil on StackOverflow '''\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for label, data in data_dict.items():\n",
    "        plt.plot(data, label=label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.legend(loc='center left')  # the plot evolves to the right\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(0, max(max(data) for data in data_dict.values()))\n",
    "    plt.show()\n",
    "\n",
    "def train(training_iterations=25):\n",
    "    # Train the model\n",
    "    learning_curve_data = defaultdict(list)\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(fingerprints_train)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, targets_train_gp)\n",
    "        loss.backward()\n",
    "        # Update the learning curve\n",
    "        learning_curve_data['Loss'].append(loss.item())\n",
    "        live_plot(learning_curve_data)\n",
    "        optimizer.step()\n",
    "\n",
    "%time train(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Performance: GP-rbf with CGCNN Mean in Transformed Output Space\n",
    "## All adsorbates\n",
    "This latest chunk of data came from multiple adsorbates. To start simple, let's just plot everything within a single pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make the predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    %time preds = model(fingerprints_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results (transformed output space)\n",
    "\n",
    "# Parse the predictions\n",
    "targets_pred_gp = preds.mean\n",
    "\n",
    "# Plot\n",
    "lims = [-4, 4]\n",
    "grid = sns.jointplot(targets_val_gp.reshape(-1),\n",
    "                     targets_pred_gp.reshape(-1),\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims*2)\n",
    "ax = grid.ax_joint\n",
    "_ = ax.set_xlim(lims)\n",
    "_ = ax.set_ylim(lims)\n",
    "_ = ax.plot(lims, lims, '--')\n",
    "_ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_val_gp, targets_pred_gp)\n",
    "rmse = np.sqrt(mean_squared_error(targets_val_gp, targets_pred_gp))\n",
    "r2 = r2_score(targets_val_gp, targets_pred_gp)\n",
    "\n",
    "# Report\n",
    "text = ('    n = %i\\n'\n",
    "        '    MAE = %.2f eV\\n'\n",
    "        '    RMSE = %.2f eV\\n'\n",
    "        '    R$^2$ = %.2f'\n",
    "        % (len(targets_val_gp), mae, rmse, r2))\n",
    "_ = ax.text(x=lims[0], y=lims[1], s=text,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Performance: GP-rbf with CGCNN Mean in Original Output Space\n",
    "## All adsorbates\n",
    "This latest chunk of data came from multiple adsorbates. To start simple, let's just plot everything within a single pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results (original output space)\n",
    "\n",
    "# Transform outputs\n",
    "targets_val_gp_orig = targets_val_gp + torch.Tensor(targets_pred)\n",
    "targets_pred_gp_orig = targets_pred_gp + torch.Tensor(targets_pred)\n",
    "\n",
    "# Plot\n",
    "lims = [-4, 4]\n",
    "grid = sns.jointplot(targets_val_gp_orig.reshape(-1),\n",
    "                     targets_pred_gp_orig.reshape(-1),\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims*2)\n",
    "ax = grid.ax_joint\n",
    "_ = ax.set_xlim(lims)\n",
    "_ = ax.set_ylim(lims)\n",
    "_ = ax.plot(lims, lims, '--')\n",
    "_ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_val_gp_orig, targets_pred_gp_orig)\n",
    "rmse = np.sqrt(mean_squared_error(targets_val_gp_orig, targets_pred_gp_orig))\n",
    "r2 = r2_score(targets_val_gp_orig, targets_pred_gp_orig)\n",
    "\n",
    "# Report\n",
    "text = ('    n = %i\\n'\n",
    "        '    MAE = %.2f eV\\n'\n",
    "        '    RMSE = %.2f eV\\n'\n",
    "        '    R$^2$ = %.2f'\n",
    "        % (len(targets_val_gp_orig), mae, rmse, r2))\n",
    "_ = ax.text(x=lims[0], y=lims[1], s=text,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing calibration\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "residuals = (targets_pred_gp_orig - targets_val_gp_orig).detach().numpy()\n",
    "stdevs = preds.stddev.detach().numpy()\n",
    "\n",
    "def calculate_density(percentile):\n",
    "    num_within_quantile = 0\n",
    "    for stdev, resid in zip(stdevs, residuals):\n",
    "        norm = stats.norm(loc=0, scale=stdev)\n",
    "        lower_bound = norm.ppf(0.5-percentile/2)\n",
    "        upper_bound = norm.ppf(0.5+percentile/2)\n",
    "        if lower_bound <= resid <= upper_bound:\n",
    "            num_within_quantile += 1\n",
    "    density = num_within_quantile / len(residuals)\n",
    "    return density\n",
    "\n",
    "predicted_pi = np.linspace(0, 1, 20)\n",
    "observed_pi = [calculate_density(quantile)\n",
    "               for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "figsize = (4, 4)\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig_cal = plt.figure(figsize=figsize)\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label=model_name)\n",
    "ax_fill = plt.fill_between(predicted_pi, predicted_pi, observed_pi,\n",
    "                           alpha=0.2, label='miscalibration area')\n",
    "_ = ax_ideal.set_xlabel('Expected prediction interval')\n",
    "_ = ax_ideal.set_ylabel('Observed prediction interval')\n",
    "_ = ax_ideal.set_xlim([0, 1])\n",
    "_ = ax_ideal.set_ylim([0, 1])\n",
    "\n",
    "# Calculate the miscalibration area.\n",
    "polygon_points = []\n",
    "for point in zip(predicted_pi, observed_pi):\n",
    "    polygon_points.append(point)\n",
    "for point in zip(reversed(predicted_pi), reversed(predicted_pi)):\n",
    "    polygon_points.append(point)\n",
    "polygon_points.append((predicted_pi[0], observed_pi[0]))\n",
    "polygon = Polygon(polygon_points)\n",
    "miscalibration_area = polygon.area\n",
    "\n",
    "# Annotate the plot with the miscalibration area\n",
    "plt.text(x=0.95, y=0.05,\n",
    "         s='Miscalibration area = %.2f' % miscalibration_area,\n",
    "         verticalalignment='bottom',\n",
    "         horizontalalignment='right')\n",
    "\n",
    "# Plot sharpness curve\n",
    "xlim = [0., 1.5]\n",
    "fig_sharp = plt.figure(figsize=figsize)\n",
    "ax_sharp = sns.distplot(stdevs, kde=False, norm_hist=True)\n",
    "ax_sharp.set_xlim(xlim)\n",
    "ax_sharp.set_xlabel('Predicted standard deviations (eV)')\n",
    "ax_sharp.set_ylabel('Normalized frequency')\n",
    "ax_sharp.set_yticklabels([])\n",
    "ax_sharp.set_yticks([])\n",
    "\n",
    "# Calculate and report sharpness\n",
    "sharpness = np.sqrt(np.mean(stdevs**2))\n",
    "_ = ax_sharp.axvline(x=sharpness, label='sharpness')\n",
    "if sharpness < (xlim[0] + xlim[1]) / 2:\n",
    "    text = '\\n  Sharpness = %.2f eV' % sharpness\n",
    "    h_align = 'left'\n",
    "else:\n",
    "    text = '\\nSharpness = %.2f eV  ' % sharpness\n",
    "    h_align = 'right'\n",
    "_ = ax_sharp.text(x=sharpness, y=ax_sharp.get_ylim()[1],\n",
    "                  s=text,\n",
    "                  verticalalignment='top',\n",
    "                  horizontalalignment=h_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results-by-adsorbate\n",
    "Let's dig into details and see how our results turned out for each adsorbate. Note that this section shows the results from a single model trained on a pooled dataset. Only the performance metrics are partitioned by adsorbate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SeabornFig2Grid():\n",
    "    '''\n",
    "    Credit goes to ImportanceOfBeingErnest on StackOverflow\n",
    "    https://stackoverflow.com/questions/35042255/how-to-plot-multiple-seaborn-jointplot-in-subplot\n",
    "    '''\n",
    "\n",
    "    def __init__(self, seaborngrid, fig, subplot_spec):\n",
    "        self.fig = fig\n",
    "        self.sg = seaborngrid\n",
    "        self.subplot = subplot_spec\n",
    "        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n",
    "            isinstance(self.sg, sns.axisgrid.PairGrid):\n",
    "            self._movegrid()\n",
    "        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n",
    "            self._movejointgrid()\n",
    "        self._finalize()\n",
    "\n",
    "    def _movegrid(self):\n",
    "        \"\"\" Move PairGrid or Facetgrid \"\"\"\n",
    "        self._resize()\n",
    "        n = self.sg.axes.shape[0]\n",
    "        m = self.sg.axes.shape[1]\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n",
    "\n",
    "    def _movejointgrid(self):\n",
    "        \"\"\" Move Jointgrid \"\"\"\n",
    "        h= self.sg.ax_joint.get_position().height\n",
    "        h2= self.sg.ax_marg_x.get_position().height\n",
    "        r = int(np.round(h/h2))\n",
    "        self._resize()\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n",
    "\n",
    "        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n",
    "\n",
    "    def _moveaxes(self, ax, gs):\n",
    "        #https://stackoverflow.com/a/46906599/4124317\n",
    "        ax.remove()\n",
    "        ax.figure=self.fig\n",
    "        self.fig.axes.append(ax)\n",
    "        self.fig.add_axes(ax)\n",
    "        ax._subplotspec = gs\n",
    "        ax.set_position(gs.get_position(self.fig))\n",
    "        ax.set_subplotspec(gs)\n",
    "\n",
    "    def _finalize(self):\n",
    "        plt.close(self.sg.fig)\n",
    "        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def _resize(self, evt=None):\n",
    "        self.sg.fig.set_size_inches(self.fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare figure for all adsorbates\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "gs = gridspec.GridSpec(1, len(adsorbates))\n",
    "\n",
    "# Parse out the data for each adsorbate\n",
    "for i, ads in enumerate(adsorbates):\n",
    "    _targets_val = []\n",
    "    _targets_pred = []\n",
    "    for doc, target, pred in zip(docs_val, targets_val_gp_orig, targets_pred_gp_orig):\n",
    "        if doc['adsorbate'] == ads:\n",
    "            _targets_val.append(target)\n",
    "            _targets_pred.append(pred)\n",
    "    _targets_val = np.array(_targets_val).reshape(-1)\n",
    "    _targets_pred = np.array(_targets_pred).reshape(-1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    lims = [-5, 5]\n",
    "    grid = sns.jointplot(_targets_val, _targets_pred,\n",
    "                         kind='hex',\n",
    "                         bins='log',\n",
    "                         extent=lims*2)\n",
    "    ax = grid.ax_joint\n",
    "    _ = ax.set_xlim(lims)\n",
    "    _ = ax.set_ylim(lims)\n",
    "    _ = ax.plot(lims, lims, '--')\n",
    "    _ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "    _ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "    _ = ax.set_title(ads)\n",
    "\n",
    "    # Calculate + report metrics\n",
    "    mae = mean_absolute_error(_targets_val, _targets_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(_targets_val, _targets_pred))\n",
    "    r2 = r2_score(_targets_val, _targets_pred)\n",
    "    grid.ax_marg_x.text(x=lims[0], y=0,\n",
    "                        verticalalignment='bottom',\n",
    "                        s='%s\\n' % ads)\n",
    "    text = ('  n = %i\\n'\n",
    "            '  MAE = %.2f eV\\n'\n",
    "            '  RMSE = %.2f eV\\n'\n",
    "            '  R$^2$ = %.2f'\n",
    "            % (len(_targets_val), mae, rmse, r2))\n",
    "    ax.text(x=lims[0], y=lims[1], s=text,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top')\n",
    "\n",
    "    # Put the figure into the subplot\n",
    "    sfg = SeabornFig2Grid(grid, fig, gs[i])\n",
    "gs.tight_layout(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
