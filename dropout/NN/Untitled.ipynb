{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, pickle, torch\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch import callbacks  # needs skorch >= 0.4  \n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import CVSplit\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/cgcnn\"))\n",
    "# from cgcnn.dropoutmodel import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print('Raw given dropout', sys.argv[1])\n",
    "# Update settings with given dropout\n",
    "if float(sys.argv[1]) != 0:\n",
    "    CrystalGraphConvNet.update_dropout(True, float(sys.argv[1]))\n",
    "\n",
    "\n",
    "# Dropout on all forward passes, in percent form:\n",
    "dropout_percent = float(sys.argv[1]) * 100\n",
    "\"\"\"\n",
    "\n",
    "# Label to use for this model in the plots\n",
    "model_name = 'CGCNN ensemble'\n",
    "\n",
    "# Load the data split from our Jupyter notebook cache\n",
    "with open('../../preprocessing/sdt/gasdb/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../../preprocessing/splits_gasdb.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "docs_train, docs_val, docs_test = splits['docs_train'], splits['docs_val'], splits['docs_test']\n",
    "sdts_train, sdts_val, sdts_test = splits['sdts_train'], splits['sdts_val'], splits['sdts_test']\n",
    "targets_train, targets_val, targets_test = splits['targets_train'], splits['targets_val'], splits['targets_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CGCNN `net` class\n",
    "# Callback to checkpoint parameters every time there is a new best for validation loss\n",
    "cp = callbacks.Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/d_valid_best_')\n",
    "\n",
    "# Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/d_valid_best_params.pt')\n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "# Callback to set the learning rate dynamically\n",
    "LR_schedule = callbacks.lr_scheduler.LRScheduler('MultiStepLR', milestones=[100], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from .module import Module # torch.nn.Module\n",
    "# from .. import functional as F # torch.nn.functional \n",
    "# from torch._jit_internal import weak_module, weak_script_method\n",
    "\n",
    "\n",
    "# New Addition 05/20 - copied from torch.nn.Modules.dropout docs\n",
    "class _DropoutNd(nn.Module):\n",
    "    __constants__ = ['p', 'inplace']\n",
    "\n",
    "    def __init__(self, p=0.5, inplace=False):\n",
    "        super(_DropoutNd, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def extra_repr(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return 'p={}{}'.format(self.p, inplace_str)\n",
    "\n",
    "    \n",
    "# @weak_module\n",
    "class AlwaysOnDropout(_DropoutNd):\n",
    "    # @weak_script_method\n",
    "    def forward(self, input):\n",
    "        return F.dropout(input, self.p, True, self.inplace)\n",
    "    \n",
    "    \n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional operation on graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_fea_len, nbr_fea_len):\n",
    "        \"\"\"\n",
    "        Initialize ConvLayer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        atom_fea_len: int\n",
    "          Number of atom hidden features.\n",
    "        nbr_fea_len: int\n",
    "          Number of bond features.\n",
    "        \"\"\"\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.atom_fea_len = atom_fea_len\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "        self.fc_full = nn.Linear(2*self.atom_fea_len+self.nbr_fea_len,\n",
    "                                 2*self.atom_fea_len)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(2*self.atom_fea_len)\n",
    "        self.bn2 = nn.BatchNorm1d(self.atom_fea_len)\n",
    "        self.softplus2 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        M: Max number of neighbors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        atom_in_fea: Variable(torch.Tensor) shape (N, atom_fea_len)\n",
    "          Atom hidden features before convolution\n",
    "        nbr_fea: Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n",
    "          Bond features of each atom's M neighbors\n",
    "        nbr_fea_idx: torch.LongTensor shape (N, M)\n",
    "          Indices of M neighbors of each atom\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        atom_out_fea: nn.Variable shape (N, atom_fea_len)\n",
    "          Atom hidden features after convolution\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO will there be problems with the index zero padding?\n",
    "        N, M = nbr_fea_idx.shape\n",
    "        # convolution\n",
    "        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\n",
    "        total_nbr_fea = torch.cat(\n",
    "            [atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n",
    "             atom_nbr_fea, nbr_fea], dim=2)\n",
    "        total_gated_fea = self.fc_full(total_nbr_fea)\n",
    "        total_gated_fea = self.bn1(total_gated_fea.view(\n",
    "            -1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2)\n",
    "        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\n",
    "        nbr_filter = self.sigmoid(nbr_filter)\n",
    "        nbr_core = self.softplus1(nbr_core)\n",
    "        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n",
    "        nbr_sumed = self.bn2(nbr_sumed)\n",
    "        out = self.softplus2(atom_in_fea + nbr_sumed)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrystalGraphConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a crystal graph convolutional neural network for predicting total\n",
    "    material properties.\n",
    "    \"\"\"\n",
    "    def __init__(self, orig_atom_fea_len, nbr_fea_len, atom_fea_len=64,\n",
    "                 n_conv=3, h_fea_len=128, n_h=1, classification=False):\n",
    "        \"\"\"\n",
    "        Initialize CrystalGraphConvNet.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        orig_atom_fea_len: int\n",
    "          Number of atom features in the input.\n",
    "        nbr_fea_len: int\n",
    "          Number of bond features.\n",
    "        atom_fea_len: int\n",
    "          Number of hidden atom features in the convolutional layers\n",
    "        n_conv: int\n",
    "          Number of convolutional layers\n",
    "        h_fea_len: int\n",
    "          Number of hidden features after pooling\n",
    "        n_h: int\n",
    "          Number of hidden layers after pooling\n",
    "        \"\"\"\n",
    "        super(CrystalGraphConvNet, self).__init__()\n",
    "        self.classification = classification\n",
    "        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n",
    "        self.convs = nn.ModuleList([ConvLayer(atom_fea_len=atom_fea_len,\n",
    "                                    nbr_fea_len=nbr_fea_len)\n",
    "                                    for _ in range(n_conv)])\n",
    "        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n",
    "        self.conv_to_fc_softplus = nn.LeakyReLU()\n",
    "        if n_h > 1:\n",
    "            self.fcs = nn.ModuleList([nn.Linear(h_fea_len, h_fea_len)\n",
    "                                      for _ in range(n_h-1)])\n",
    "            self.softpluses = nn.ModuleList([nn.LeakyReLU()\n",
    "                                             for _ in range(n_h-1)])\n",
    "            self.bn = nn.ModuleList([nn.BatchNorm1d(h_fea_len)\n",
    "                                             for _ in range(n_h-1)])\n",
    "        if self.classification:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 2)\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 1)\n",
    "        if self.classification:\n",
    "            self.logsoftmax = nn.LogSoftmax()\n",
    "            self.dropout = nn.Dropout()\n",
    "            \n",
    "        self.to('cuda',non_blocking=True)\n",
    "        for module in self.convs:\n",
    "            module.to('cuda',non_blocking=True)\n",
    "        \n",
    "        if not hasattr(self, 'dropout_bool_') or not hasattr(self, 'dropout_weight_'):\n",
    "            self.dropout_bool_ = False\n",
    "            self.dropout_weight_ = 0.0\n",
    "\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def update_dropout(cls, dropout_bool, dropout_weight):\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        dropout_bool_: if dropout is applied to forward passes\n",
    "        dropout_weight_: weight of dropout if dropout is applied to \n",
    "          forward passes\n",
    "        \n",
    "        cls.dropout_bool_ = dropout_bool\n",
    "        cls.dropout_weight_ = dropout_weight\n",
    "    \"\"\"\n",
    "        \n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        M: Max number of neighbors\n",
    "        N0: Total number of crystals in the batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        atom_fea: Variable(torch.Tensor) shape (N, orig_atom_fea_len)\n",
    "          Atom features from atom type\n",
    "        nbr_fea: Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n",
    "          Bond features of each atom's M neighbors\n",
    "        nbr_fea_idx: torch.LongTensor shape (N, M)\n",
    "          Indices of M neighbors of each atom\n",
    "        crystal_atom_idx: list of torch.LongTensor of length N0\n",
    "          Mapping from the crystal idx to atom idx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        prediction: nn.Variable shape (N, )\n",
    "          Atom hidden features after convolution\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" Dropout should be put here \"\"\"\n",
    "        dropout  = AlwaysOnDropout(self.dropout_weight_)\n",
    "        \n",
    "        atom_fea = self.embedding(atom_fea)\n",
    "        for conv_func in self.convs:\n",
    "            atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "        crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n",
    "        crys_fea = self.conv_to_fc_softplus(crys_fea)\n",
    "        if self.classification:\n",
    "            crys_fea = self.dropout(crys_fea)\n",
    "        if hasattr(self, 'fcs') and hasattr(self, 'softpluses'):\n",
    "            for fc, softplus,bn in zip(self.fcs, self.softpluses, self.bn):\n",
    "                if self.dropout_bool_:\n",
    "                    crys_fea = dropout(softplus(bn(fc(crys_fea)))) # with dropout\n",
    "                else:\n",
    "                    crys_fea = softplus(bn(fc(crys_fea))) # no dropout\n",
    "        out = self.fc_out(crys_fea)\n",
    "        if self.classification:\n",
    "            out = self.logsoftmax(out)\n",
    "        return out\n",
    "\n",
    "    def pooling(self, atom_fea, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Pooling the atom features to crystal features\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        N0: Total number of crystals in the batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        atom_fea: Variable(torch.Tensor) shape (N, atom_fea_len)\n",
    "          Atom feature vectors of the batch\n",
    "        crystal_atom_idx: list of torch.LongTensor of length N0\n",
    "          Mapping from the crystal idx to atom idx\n",
    "        \"\"\"\n",
    "        assert sum([len(idx_map) for idx_map in crystal_atom_idx]) ==\\\n",
    "            atom_fea.data.shape[0]\n",
    "        summed_fea = [torch.mean(atom_fea[idx_map], dim=0, keepdim=True)\n",
    "                      for idx_map in crystal_atom_idx]\n",
    "        return torch.cat(summed_fea, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "    module__nbr_fea_len=nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=150,\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn=collate_pool,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn=collate_pool,\n",
    "    iterator_valid__shuffle=False,\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    dropoout_bool_=True,\n",
    "    dropout_weight_=0.20,\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"./d_cgcnn.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaspy_ktran",
   "language": "python",
   "name": "gaspy_ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
