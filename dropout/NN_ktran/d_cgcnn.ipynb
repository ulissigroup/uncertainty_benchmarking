{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to train, validate and test a single, plain CGCNN model as shown in https://github.com/ulissigroup/cgcnn/tree/dropout/.\n",
    "\n",
    "All the code contained in this notebook, from training to plot generation, can be run using a single line of Unix command:\n",
    "\n",
    "```python d_cgcnn.py 0.20```\n",
    "\n",
    "Here, `0.20` stands for the amount of dropout we wish to apply to the training, validation and testing stages of the ensembling. To examine different amounts of dropout at the same time, please use the batch file `submit_cgcnn_fitting.sh`, to run multiple instances of `d_cgcnn.py` with different dropout values in parallel.\n",
    "\n",
    "To do this, open a terminal, change the working directory to the this folder, and type in the terminal\n",
    "\n",
    "```\n",
    "module load esslurm\n",
    "sbatch submit_cgcnn_fitting.sh\n",
    "```\n",
    "\n",
    "Alternatively, for single training tasks, you can specify the desired dropout amount in this notebook, by modifying the definitions of the neural network used here. For example, if you wish to train a CGCNN network with dropout level 0.30, you can feed the following arguments to the `NeuralNetRegressor()` instance found below:\n",
    "\n",
    "```\n",
    "net = NeuralNetRegressor(...\n",
    "                         dropout_bool_=True,\n",
    "                         dropout_weight_=0.30,\n",
    "                         ...)\n",
    "```\n",
    "\n",
    "If the dropout is not specified, it will be set to a default of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify here the amount of dropout you wish to apply to the training, validation and testing stages of the ensembling. A default value of 0.20 is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.20\n",
    "dropout_percent = int(dropout * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to apply a dropout of 0.0, dropout_bool is set to `False`. This is the default option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_bool = (dropout != False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically search for an NVIDIA GPU and use it. If not, then use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "\n",
    "# Find and use the appropriate GPU/CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of our preprocessed data from the caches that we generated with the `../../preprocessing/sdt/gasdb/create_sdt.py` and `../../preprocessing/split_data_gasdb.ipynb` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Label to use for this model in the plots\n",
    "model_name = 'CGCNN standalone'\n",
    "\n",
    "with open('../../preprocessing/sdt/gasdb/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../../preprocessing/splits_gasdb.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "docs_train, docs_val, docs_test = splits['docs_train'], splits['docs_val'], splits['docs_test']\n",
    "sdts_train, sdts_val, sdts_test = splits['sdts_train'], splits['sdts_val'], splits['sdts_test']\n",
    "targets_train, targets_val, targets_test = splits['targets_train'], splits['targets_val'], splits['targets_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes an hour to fit our dataset per CGCNN model, using Intel Xeon Gold 6148 CPUs at 2.40GHz, and Tesla V100-SXM2 GPUs with 16 GB memory. Since we have to repeat the same fitting process for multiple dropout levels, we used a Python script and submitted it as a \"job\" on our high-performing-computer center via `sbatch submit_cgcnn_fitting.sh`. \n",
    "\n",
    "Since we examined 7 distinct dropout levels, we intend to run 7 parallel fitting processes.\n",
    "\n",
    "Here is what the Python script contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# # Initialization\n",
    "# Importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, pickle, torch\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch import callbacks  # needs skorch >= 0.4  \n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import CVSplit\n",
    "from cgcnn.dropoutmodel import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "sys.path.append(os.path.expanduser(\"~/cgcnn\"))\n",
    "\n",
    "\n",
    "# Define the dropout on all forward passes\n",
    "dropout = 0.20\n",
    "dropout_percent = float(dropout) * 100 # in percent form\n",
    "print('Raw given dropout', dropout)\n",
    "\n",
    "\n",
    "# Automatically search for an NVIDIA GPU and use it. If not, then use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "\n",
    "# Load all of our preprocessed and split data from our cache\n",
    "model_name = 'CGCNN standalone'\n",
    "\n",
    "with open('../../preprocessing/sdt/gasdb/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../../preprocessing/splits_gasdb.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "docs_train, docs_val, docs_test = splits['docs_train'], splits['docs_val'], splits['docs_test']\n",
    "sdts_train, sdts_val, sdts_test = splits['sdts_train'], splits['sdts_val'], splits['sdts_test']\n",
    "targets_train, targets_val, targets_test = splits['targets_train'], splits['targets_val'], splits['targets_test']\n",
    "\n",
    "\n",
    "# Initialize the CGCNN `net` class\n",
    "# Callback to checkpoint parameters every time there is a new best for validation loss\n",
    "cp = callbacks.Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/d%i_valid_best_' % dropout_percent)\n",
    "\n",
    "# Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/d%i_valid_best_params.pt' % dropout_percent,)\n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "# Callback to set the learning rate dynamically\n",
    "LR_schedule = callbacks.lr_scheduler.LRScheduler('MultiStepLR', milestones=[100], gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "    module__nbr_fea_len=nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=150,\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn=collate_pool,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn=collate_pool,\n",
    "    iterator_valid__shuffle=False,\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    dropout_bool_=dropout_bool,\n",
    "    dropout_weight_=dropout,\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")\n",
    "\n",
    "\n",
    "# # Training\n",
    "\n",
    "# We can train a new model...\n",
    "f_history   = './histories/valid_best_d%i_history.json' % dropout_percent\n",
    "f_optimizer = './histories/valid_best_d%i_optimizer.pt' % dropout_percent\n",
    "f_params    = './histories/valid_best_d%i_params.pt' % dropout_percent\n",
    "\n",
    "# We can save time by using previously cached parameters\n",
    "if (os.path.exists(f_history) and \n",
    "    os.path.exists(f_optimizer) and \n",
    "    os.path.exists(f_params) and\n",
    "    retrain == False):\n",
    "    net.initialize()\n",
    "    net.load_params(f_history=f_history,\n",
    "                    f_optimizer=f_optimizer, \n",
    "                    f_params=f_params)\n",
    "else:\n",
    "    net.initialize()\n",
    "    net.fit(sdts_train, targets_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models\n",
    "We load the parameters generated from the long training process here.\n",
    "\n",
    "This requires a modified version of the CGCNN package, which is available at `https://github.com/ulissigroup/cgcnn/tree/dropout`.  We assume this version of the CGCNN module is installed under the `$HOME/cgcnn` directory. If this is not the case, please modify the first line in the cell below to the directory where CGCNN is installed, as well as the ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialization\n",
    "# Importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, pickle, torch\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch import callbacks  # needs skorch >= 0.4  \n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import CVSplit\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/cgcnn\"))\n",
    "from cgcnn.dropoutmodel import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "# Initialize the CGCNN `net` class\n",
    "# Callback to checkpoint parameters every time there is a new best for validation loss\n",
    "cp = callbacks.Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/d%i_valid_best_' % dropout_percent)\n",
    "\n",
    "# Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./d%i_valid_best_params.pt' % dropout_percent,)\n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "# Callback to set the learning rate dynamically\n",
    "LR_schedule = callbacks.lr_scheduler.LRScheduler('MultiStepLR', milestones=[100], gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "    module__nbr_fea_len=nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=150,\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn=collate_pool,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn=collate_pool,\n",
    "    iterator_valid__shuffle=False,\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    dropout_bool_=dropout_bool,\n",
    "    dropout_weight_=dropout,\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.load_params(f_history='./valid_best_d%i_history.json' % dropout_percent,\n",
    "                f_optimizer='./valid_best_d%i_optimizer.pt' % dropout_percent, \n",
    "                f_params='./valid_best_d%i_params.pt' % dropout_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set figure defaults\n",
    "width = 7.5/3  # 1/3 of a page\n",
    "fontsize = 20\n",
    "rc = {'figure.figsize': (width, width),\n",
    "      'font.size': fontsize,\n",
    "      'axes.labelsize': fontsize,\n",
    "      'axes.titlesize': fontsize,\n",
    "      'xtick.labelsize': fontsize,\n",
    "      'ytick.labelsize': fontsize,\n",
    "      'legend.fontsize': fontsize}\n",
    "sns.set(rc=rc)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8982d4d2c1b445f49c877eca93dc9d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_squared_error,\n",
    "                             r2_score,\n",
    "                             median_absolute_error)\n",
    "\n",
    "\n",
    "# Make the predictions\n",
    "predy_val = np.array([net.predict(sdts_test) for _ in tqdm_notebook(range(100))])\n",
    "targets_pred = predy_val.mean(axis=1)\n",
    "residuals = targets_pred - targets_val.reshape(-1)\n",
    "stdevs = predy_val.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b4e5675b641b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hex'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                      extent=lims+lims)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_joint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/homes/k/ktran/miniconda3/envs/gaspy/lib/python3.6/site-packages/seaborn/axisgrid.py\u001b[0m in \u001b[0;36mjointplot\u001b[0;34m(x, y, data, kind, stat_func, color, height, ratio, space, dropna, xlim, ylim, joint_kws, marginal_kws, annot_kws, **kwargs)\u001b[0m\n\u001b[1;32m   2282\u001b[0m     grid = JointGrid(x, y, data, dropna=dropna,\n\u001b[1;32m   2283\u001b[0m                      \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                      xlim=xlim, ylim=ylim)\n\u001b[0m\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[0;31m# Plot the data using the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/homes/k/ktran/miniconda3/envs/gaspy/lib/python3.6/site-packages/seaborn/axisgrid.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, data, height, ratio, space, dropna, xlim, ylim, size)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0mnot_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m             \u001b[0mx_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_na\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m             \u001b[0my_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_na\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "lims = [-3, 3]\n",
    "grid = sns.jointplot(targets_val.reshape(-1), targets_pred,\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims+lims)\n",
    "ax = grid.ax_joint\n",
    "_ = ax.set_xlim(lims)\n",
    "_ = ax.set_ylim(lims)\n",
    "_ = ax.plot(lims, lims, '--')\n",
    "_ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_val, targets_pred)\n",
    "rmse = np.sqrt(mean_squared_error(targets_val, targets_pred))\n",
    "mdae = median_absolute_error(targets_val, targets_pred)\n",
    "marpd = np.abs(2 * residuals /\n",
    "               (np.abs(targets_pred) + np.abs(targets_val.reshape(-1)))\n",
    "               ).mean() * 100\n",
    "r2 = r2_score(targets_val, targets_pred)\n",
    "corr = np.corrcoef(targets_val.reshape(-1), targets_pred)[0, 1]\n",
    "\n",
    "# Report\n",
    "text = ('  MDAE = %.2f eV\\n' % mdae + \n",
    "        '  MAE = %.2f eV\\n' % mae + \n",
    "        '  RMSE = %.2f eV\\n' % rmse + \n",
    "        '  MARPD = %i%%\\n' % marpd)\n",
    "print('R2 = %.2f' % r2)\n",
    "print('PPMCC = %.2f' % corr)\n",
    "_ = ax.text(x=lims[0], y=lims[1], s=text,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/ktran/miniconda3/envs/gaspy/lib/python3.6/site-packages/ipykernel_launcher.py:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e40ce6ff544e148f5c103bf9633bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Calibration', style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (663100,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6fc1e00c17a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpredicted_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m observed_pi = [calculate_density(quantile)\n\u001b[0;32m---> 36\u001b[0;31m                for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6fc1e00c17a7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpredicted_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m observed_pi = [calculate_density(quantile)\n\u001b[0;32m---> 36\u001b[0;31m                for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6fc1e00c17a7>\u001b[0m in \u001b[0;36mcalculate_density\u001b[0;34m(percentile)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Normalize the residuals so they all should fall on the normal bell curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mnormalized_residuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstdevs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Count how many residuals fall inside here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (663100,) (100,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "# Define a normalized bell curve we'll be using to calculate calibration\n",
    "norm = stats.norm(loc=0, scale=1)\n",
    "\n",
    "\n",
    "def calculate_density(percentile):\n",
    "    '''\n",
    "    Calculate the fraction of the residuals that fall within the center\n",
    "    `percentile` of their respective Gaussian distributions, which are\n",
    "    defined by their respective uncertainty estimates.\n",
    "    '''\n",
    "    # Find the normalized bounds of this percentile\n",
    "    lower_bound = norm.ppf(0.5-percentile/2)\n",
    "    upper_bound = norm.ppf(0.5+percentile/2)\n",
    "\n",
    "    # Normalize the residuals so they all should fall on the normal bell curve\n",
    "    normalized_residuals = residuals.reshape(-1) / stdevs.reshape(-1)\n",
    "\n",
    "    # Count how many residuals fall inside here\n",
    "    num_within_quantile = 0\n",
    "    for resid in normalized_residuals:\n",
    "        if lower_bound <= resid <= upper_bound:\n",
    "            num_within_quantile += 1\n",
    "\n",
    "    # Return the fraction of residuals that fall within the bounds\n",
    "    density = num_within_quantile / len(residuals)\n",
    "    return density\n",
    "\n",
    "\n",
    "predicted_pi = np.linspace(0, 1, 100)\n",
    "observed_pi = [calculate_density(quantile)\n",
    "               for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set figure defaults\n",
    "width = 4  # Because it looks good\n",
    "fontsize = 12\n",
    "rc = {'figure.figsize': (width, width),\n",
    "      'font.size': fontsize,\n",
    "      'axes.labelsize': fontsize,\n",
    "      'axes.titlesize': fontsize,\n",
    "      'xtick.labelsize': fontsize,\n",
    "      'ytick.labelsize': fontsize,\n",
    "      'legend.fontsize': fontsize}\n",
    "sns.set(rc=rc)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from shapely.ops import polygonize, unary_union\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "figsize = (width, width)\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig_cal = plt.figure(figsize=figsize)\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label=model_name)\n",
    "ax_fill = plt.fill_between(predicted_pi, predicted_pi, observed_pi,\n",
    "                           alpha=0.2, label='miscalibration area')\n",
    "_ = ax_ideal.set_xlabel('Expected prediction interval')\n",
    "_ = ax_ideal.set_ylabel('Observed prediction interval')\n",
    "_ = ax_ideal.set_xlim([0, 1])\n",
    "_ = ax_ideal.set_ylim([0, 1])\n",
    "\n",
    "# Calculate the miscalibration area.\n",
    "polygon_points = []\n",
    "for point in zip(predicted_pi, observed_pi):\n",
    "    polygon_points.append(point)\n",
    "for point in zip(reversed(predicted_pi), reversed(predicted_pi)):\n",
    "    polygon_points.append(point)\n",
    "polygon_points.append((predicted_pi[0], observed_pi[0]))\n",
    "polygon = Polygon(polygon_points)\n",
    "x, y = polygon.exterior.xy # original data\n",
    "ls = LineString(np.c_[x, y]) # closed, non-simple\n",
    "lr = LineString(ls.coords[:] + ls.coords[0:1])\n",
    "mls = unary_union(lr)\n",
    "polygon_area_list =[poly.area for poly in polygonize(mls)]\n",
    "miscalibration_area = np.asarray(polygon_area_list).sum()\n",
    "\n",
    "# Annotate the plot with the miscalibration area\n",
    "plt.text(x=0.95, y=0.05,\n",
    "         s='Miscalibration area = %.2f' % miscalibration_area,\n",
    "         verticalalignment='bottom',\n",
    "         horizontalalignment='right',\n",
    "         fontsize=fontsize)\n",
    "\n",
    "\n",
    "# Plot sharpness curve\n",
    "xlim = [0, 1.]\n",
    "fig_sharp = plt.figure(figsize=figsize)\n",
    "ax_sharp = sns.distplot(stdevs, kde=False, norm_hist=True)\n",
    "ax_sharp.set_xlim(xlim)\n",
    "ax_sharp.set_xlabel('Predicted standard deviation (eV)')\n",
    "ax_sharp.set_ylabel('Normalized frequency')\n",
    "ax_sharp.set_yticklabels([])\n",
    "ax_sharp.set_yticks([])\n",
    "\n",
    "# Calculate and report sharpness\n",
    "sharpness = np.sqrt(np.mean(stdevs**2))\n",
    "_ = ax_sharp.axvline(x=sharpness, label='sharpness')\n",
    "if sharpness < (xlim[0] + xlim[1]) / 2:\n",
    "    text = '\\n  Sharpness = %.2f eV' % sharpness\n",
    "    h_align = 'left'\n",
    "else:\n",
    "    text = '\\nSharpness = %.2f eV  ' % sharpness\n",
    "    h_align = 'right'\n",
    "_ = ax_sharp.text(x=sharpness, y=ax_sharp.get_ylim()[1],\n",
    "                  s=text,\n",
    "                  verticalalignment='top',\n",
    "                  horizontalalignment=h_align,\n",
    "                  fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "### Parity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "predy_test = np.array([net.predict(sdts_test) for _ in tqdm_notebook(range(100))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set figure defaults\n",
    "width = 4  # Because it looks good\n",
    "fontsize = 12\n",
    "rc = {'figure.figsize': (width, width),\n",
    "      'font.size': fontsize,\n",
    "      'axes.labelsize': fontsize,\n",
    "      'axes.titlesize': fontsize,\n",
    "      'xtick.labelsize': fontsize,\n",
    "      'ytick.labelsize': fontsize,\n",
    "      'legend.fontsize': fontsize}\n",
    "sns.set(rc=rc)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_squared_error,\n",
    "                             r2_score,\n",
    "                             median_absolute_error)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Make the predictions\n",
    "targets_pred = predy_test.mean(axis=1)\n",
    "residuals = targets_pred - targets_test.reshape(-1)\n",
    "stdevs = predy_test.std(axis=1)\n",
    "\n",
    "# Plot\n",
    "lims = [-4, 4]\n",
    "grid = sns.jointplot(targets_test.reshape(-1), targets_pred,\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims+lims)\n",
    "ax = grid.ax_joint\n",
    "_ = ax.set_xlim(lims)\n",
    "_ = ax.set_ylim(lims)\n",
    "_ = ax.plot(lims, lims, '--')\n",
    "_ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_test, targets_pred)\n",
    "rmse = np.sqrt(mean_squared_error(targets_test, targets_pred))\n",
    "mdae = median_absolute_error(targets_test, targets_pred)\n",
    "marpd = np.abs(2 * residuals /\n",
    "               (np.abs(targets_pred) + np.abs(targets_test.reshape(-1)))\n",
    "               ).mean() * 100\n",
    "r2 = r2_score(targets_test, targets_pred)\n",
    "corr = np.corrcoef(targets_test.reshape(-1), targets_pred)[0, 1]\n",
    "\n",
    "# Report\n",
    "text = ('  MDAE = %.2f eV\\n' % mdae + \n",
    "        '  MAE = %.2f eV\\n' % mae + \n",
    "        '  RMSE = %.2f eV\\n' % rmse + \n",
    "        '  MARPD = %i%%\\n' % marpd)\n",
    "print('R2 = %.2f' % r2)\n",
    "print('PPMCC = %.2f' % corr)\n",
    "_ = ax.text(x=lims[0], y=lims[1], s=text,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('parity.pdf', dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "figsize = (4, 4)\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig_cal = plt.figure(figsize=figsize)\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label=model_name)\n",
    "ax_fill = plt.fill_between(predicted_pi, predicted_pi, observed_pi,\n",
    "                           alpha=0.2, label='miscalibration area')\n",
    "_ = ax_ideal.set_xlabel('Expected prediction interval')\n",
    "_ = ax_ideal.set_ylabel('Observed prediction interval')\n",
    "_ = ax_ideal.set_xlim([0, 1])\n",
    "_ = ax_ideal.set_ylim([0, 1])\n",
    "\n",
    "# Calculate the miscalibration area.\n",
    "polygon_points = []\n",
    "for point in zip(predicted_pi, observed_pi):\n",
    "    polygon_points.append(point)\n",
    "for point in zip(reversed(predicted_pi), reversed(predicted_pi)):\n",
    "    polygon_points.append(point)\n",
    "polygon_points.append((predicted_pi[0], observed_pi[0]))\n",
    "polygon = Polygon(polygon_points)\n",
    "miscalibration_area = polygon.area\n",
    "\n",
    "# Annotate the plot with the miscalibration area\n",
    "plt.text(x=0.95, y=0.05,\n",
    "         s='Miscalibration area = %.2f' % miscalibration_area,\n",
    "         verticalalignment='bottom',\n",
    "         horizontalalignment='right')\n",
    "\n",
    "# Save\n",
    "plt.savefig('calibration.pdf', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "\n",
    "# Plot sharpness curve\n",
    "xlim = [0, 1.]\n",
    "fig_sharp = plt.figure(figsize=figsize)\n",
    "ax_sharp = sns.distplot(standard_errors, kde=False, norm_hist=True)\n",
    "ax_sharp.set_xlim(xlim)\n",
    "ax_sharp.set_xlabel('Predicted standard deviation (eV)')\n",
    "ax_sharp.set_ylabel('Normalized frequency')\n",
    "ax_sharp.set_yticklabels([])\n",
    "ax_sharp.set_yticks([])\n",
    "\n",
    "# Calculate and report sharpness\n",
    "sharpness = np.sqrt(np.mean(standard_errors**2))\n",
    "_ = ax_sharp.axvline(x=sharpness, label='sharpness')\n",
    "if sharpness < (xlim[0] + xlim[1]) / 2:\n",
    "    text = '\\n  Sharpness = %.2f eV' % sharpness\n",
    "    h_align = 'left'\n",
    "else:\n",
    "    text = '\\nSharpness = %.2f eV  ' % sharpness\n",
    "    h_align = 'right'\n",
    "_ = ax_sharp.text(x=sharpness, y=ax_sharp.get_ylim()[1],\n",
    "                  s=text,\n",
    "                  verticalalignment='top',\n",
    "                  horizontalalignment=h_align)\n",
    "\n",
    "# Save\n",
    "# plt.savefig('sharpness.pdf', dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set figure defaults\n",
    "width = 4  # Because it looks good\n",
    "fontsize = 12\n",
    "rc = {'figure.figsize': (width, width),\n",
    "      'font.size': fontsize,\n",
    "      'axes.labelsize': fontsize,\n",
    "      'axes.titlesize': fontsize,\n",
    "      'xtick.labelsize': fontsize,\n",
    "      'ytick.labelsize': fontsize,\n",
    "      'legend.fontsize': fontsize}\n",
    "sns.set(rc=rc)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from shapely.ops import polygonize, unary_union\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "figsize = (width, width)\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig_cal = plt.figure(figsize=figsize)\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label=model_name)\n",
    "ax_fill = plt.fill_between(predicted_pi, predicted_pi, observed_pi,\n",
    "                           alpha=0.2, label='miscalibration area')\n",
    "_ = ax_ideal.set_xlabel('Expected prediction interval')\n",
    "_ = ax_ideal.set_ylabel('Observed prediction interval')\n",
    "_ = ax_ideal.set_xlim([0, 1])\n",
    "_ = ax_ideal.set_ylim([0, 1])\n",
    "\n",
    "# Calculate the miscalibration area.\n",
    "polygon_points = []\n",
    "for point in zip(predicted_pi, observed_pi):\n",
    "    polygon_points.append(point)\n",
    "for point in zip(reversed(predicted_pi), reversed(predicted_pi)):\n",
    "    polygon_points.append(point)\n",
    "polygon_points.append((predicted_pi[0], observed_pi[0]))\n",
    "polygon = Polygon(polygon_points)\n",
    "x, y = polygon.exterior.xy # original data\n",
    "ls = LineString(np.c_[x, y]) # closed, non-simple\n",
    "lr = LineString(ls.coords[:] + ls.coords[0:1])\n",
    "mls = unary_union(lr)\n",
    "polygon_area_list =[poly.area for poly in polygonize(mls)]\n",
    "miscalibration_area = np.asarray(polygon_area_list).sum()\n",
    "\n",
    "# Annotate the plot with the miscalibration area\n",
    "plt.text(x=0.95, y=0.05,\n",
    "         s='Miscalibration area = %.2f' % miscalibration_area,\n",
    "         verticalalignment='bottom',\n",
    "         horizontalalignment='right',\n",
    "         fontsize=fontsize)\n",
    "\n",
    "\n",
    "# Plot sharpness curve\n",
    "xlim = [0, 1.]\n",
    "fig_sharp = plt.figure(figsize=figsize)\n",
    "ax_sharp = sns.distplot(stdevs, kde=False, norm_hist=True)\n",
    "ax_sharp.set_xlim(xlim)\n",
    "ax_sharp.set_xlabel('Predicted standard deviation (eV)')\n",
    "ax_sharp.set_ylabel('Normalized frequency')\n",
    "ax_sharp.set_yticklabels([])\n",
    "ax_sharp.set_yticks([])\n",
    "\n",
    "# Calculate and report sharpness\n",
    "sharpness = np.sqrt(np.mean(stdevs**2))\n",
    "_ = ax_sharp.axvline(x=sharpness, label='sharpness')\n",
    "if sharpness < (xlim[0] + xlim[1]) / 2:\n",
    "    text = '\\n  Sharpness = %.2f eV' % sharpness\n",
    "    h_align = 'left'\n",
    "else:\n",
    "    text = '\\nSharpness = %.2f eV  ' % sharpness\n",
    "    h_align = 'right'\n",
    "_ = ax_sharp.text(x=sharpness, y=ax_sharp.get_ylim()[1],\n",
    "                  s=text,\n",
    "                  verticalalignment='top',\n",
    "                  horizontalalignment=h_align,\n",
    "                  fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-bar figure\n",
    "We concede that calibration curves and sharpness distributions are new concepts in the field of catalysis, and that a simple parity plot with error bars is more intuitive. As such, we create a few examples of error bar parities to help readers connect those incumbent ideas with the newer ideas of calibration and sharpness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pull a random sample of the data, because plotting thousands of these at once would look absurd\n",
    "all_predictions = list(zip(targets_pred, targets_test.reshape(-1), stdevs))\n",
    "samples = random.sample(all_predictions, k=20)\n",
    "\n",
    "# Parse the samples\n",
    "_preds, _targets, _stdevs = zip(*samples)\n",
    "_preds = np.array(_preds)\n",
    "_targets = np.array(_targets)\n",
    "_stdevs = np.array(_stdevs)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "_ = plt.errorbar(_targets, _preds, yerr=2*_stdevs, fmt='o')\n",
    "ax = plt.gca()\n",
    "\n",
    "# Make a parity line\n",
    "lims = [-2, 2]\n",
    "_ = ax.plot(lims, lims, '--')\n",
    "\n",
    "# Format\n",
    "_ = ax.set_xlim(lims)\n",
    "_ = ax.set_ylim(lims)\n",
    "_ = ax.set_xticks(list(range(-2, 3)))\n",
    "_ = ax.set_yticks(list(range(-2, 3)))\n",
    "_ = ax.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = ax.set_ylabel('%s $\\Delta$E [eV]' % model_name)\n",
    "\n",
    "# Save\n",
    "_ = plt.savefig('error_bar_parity.pdf', dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data\n",
    "We need to save the prediction data with the `pickle` module, so we can compare the effects of different prediction methods and different dropout levels in one plot, using another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle to be plotted with in the same graph as others\n",
    "with open('assess_ensemble_d%i.pkl' % dropout_int, 'wb') as saveplot:\n",
    "    pickle.dump((predictions, targets_val), saveplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaspy_ktran",
   "language": "python",
   "name": "gaspy_ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
