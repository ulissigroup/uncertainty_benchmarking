\documentclass[]{achemso}

\usepackage{amsmath}        % Equation editing using flags of \begin{align} and \end{align}
\usepackage{commath}        % We have this here to use the \abs{} equation function
\usepackage{graphicx}       % Display figures using \includegraphics
\graphicspath{{figures/}}   % Location for figures relative to .tex file path
\usepackage{subcaption}     % Enable the \subfigure command for multi-paneled figures
\usepackage{booktabs}       % Use table things like \toprule or \bottomrule
\usepackage[table]{xcolor}  % Lets us use \rowcolors to make alternate table shading
\usepackage{makecell}       % Allow the use of the \makecell command that lets linebreaks in table cells
\usepackage{etoolbox}       % Make the bibliograph unjustified (to work around hbox errors)
\apptocmd{\thebibliography}{\raggedright}{}{}
\usepackage{glossaries}     % Enable the \newacronym and \gls commands to reference terms

\newacronym{DFT}{DFT}{Density Functional Theory}
\newacronym{VASP}{VASP}{the Vienna Ab-initio Simulation Package}
\newacronym{GASpy}{GASpy}{the Generalized Adsorption Simulator for Python}
\newacronym{rPBE}{rPBE}{revised Perdew-Burke-Ernzerhof}
\newacronym{ASE}{ASE}{the Atomic Simulation Environment}
\newacronym{NEB}{NEB}{nudged elastic band}
\newacronym{GP}{GP}{Gaussian Process}
\newacronym{UQ}{UQ}{uncertainty quantification}
\newacronym{MAE}{MAE}{Mean Absolute Error}
\newacronym{RMSE}{RMSE}{Root Mean Squared Error}
\newacronym{MDAE}{MDAE}{Median Absolute Error}
\newacronym{MARPD}{MARPD}{Mean Absolute Relative Percent Difference}
\newacronym{R2}{R\textsuperscript{2}}{R\textsuperscript{2} correlation coefficient}
\newacronym{NN}{NN}{crystal graph convolutional Neural Network}
\newacronym{BNN}{BNN}{Bayesian Neural Network}
\newacronym{MLE}{MLE}{Maximum Likelihood Estimation}
\newacronym{CFGP}{CFGP}{Convolution-Fed Gaussian Process}
\newacronym{dNN}{NN$\Delta$NN}{in-series NNs}
\newacronym{dGP}{NN$\Delta$GP}{in-series NN \& GP}
\newacronym{ML}{ML}{Machine Learning}
\newacronym{eV}{eV}{electron volts}
\newacronym{NLL}{NLL}{negative log-likelihood}


%%%%%%%%%%%%%%%%%%%% Title/Abstract %%%%%%%%%%%%%%%%%%%%
\title{Methods for comparing uncertainty quantifications for material property predictions}
\author{Kevin Tran}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\altaffiliation{These authors contributed equally to this work}
\author{Willie Neiswanger}
\affiliation{Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\altaffiliation{These authors contributed equally to this work}
\author{Junwoong Yoon}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\author{Eric Xing}
\affiliation{Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\author{Zachary W. Ulissi}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\email{zulissi@andrew.cmu.edu}

\begin{document}

%\setlength{\fboxrule}{0 pt}
%\begin{tocentry}
%    \includegraphics[width=\textwidth]{TOC/TOC.pdf}
%    TOC caption here.
%\end{tocentry}

\begin{abstract}
    Abstract here.
\end{abstract}


%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The fields of catalysis and materials science are burgeoning with methods to screen, design, and understand materials.\cite{Medford2018, Gu2019, Schleder2019, Alberi2019}
This research has spurned the creation of \gls{ML} models to predict various material properties.
Unfortunately, the design spaces for these models are sometimes too large and intractable to sample completely.
These undersampling issues can limit the training data and therefore the predictive power of the models.
It would be helpful to have an \gls{UQ} for a model so that we know when to trust the predictions and when not to.
More specifically:  \gls{UQ} would enable various online, active frameworks for materials discovery and design (e.g., active learning,\cite{Settles2012} online active learning,\cite{Chu2011} Bayesian optimization,\cite{Frazier2018} active search,\cite{Garnett2012} or goal oriented design of experiments\cite{Kandasamy}).

Such active frameworks have already been used successfully in the field of catalysis and materials informatics.
For example: \citet{Peterson2016} has used a neural network to perform online active learning of \gls{NEB} calculations, reducing the number of force calls by an order of magnitude.
\citet{Torres2018} have also used online active learning to accelerate \gls{NEB} calculations, but they used a \gls{GP} model instead of a neural network.
\citet{Jinnouchi2019} have used online active learning to accelerate molecular dynamics simulations.
Each of these active methods are underpinned by models with \gls{UQ}, which have garnered increasing attention themselves.\cite{Peterson2017, Musil2019}

The goal of \gls{UQ} in predictive models is to quantify the relative likelihood of potential true outcomes associated with a predicted quantity, and to accurately assess the probabilities of these outcomes.
For example, given an input for which we wish to make a prediction, a predictive \gls{UQ} method might return a confidence interval that aims to capture the true outcome a specified percentage of the time, or might return a probability distribution over possible outcomes.
Performance metrics for predictive \gls{UQ} methods aim to assess how well a given quantification of the probabilities of potential true outcomes adheres to a set of observations of these outcomes.
Some of the performance metrics for predictive \gls{UQ} are agnostic to prediction performance---they provide an assessment of the uncertainty independent of the predictive accuracy (i.e.\ a method can predict badly, but could still accurately quantify its own uncertainty).

To our knowledge though, we have seen few\cite{Janet2019} comparisons of different methods for \gls{UQ} within the field of catalysis and materials informatics.
Here we attempt to establish a protocol for comparing the performance of different modeling and \gls{UQ} methods (Figure~\ref{fig:overview}).
We then illustrate the protocol on a case study where we compare various models' abilities to predict \gls{DFT} calculated adsorption energies.
We also offer anecdotal insights from our case study.
We acknowledge that such insights may not be transferable to other applications, but we still find value in sharing these results so that others can build their own intuition from them.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{intro/intro.pdf}
    \caption{Overview of proposed procedure for judging the quality of models with uncerainty estimates.
    First and foremost, the models should be accurate.
    Second, the models should be ``calibrated'', which means that their uncertainty estimates should be comparable with their residuals.
    Third, the models should be ``sharp'', which means that their uncertainty estimates should be narrow.
    This study will show how to visualize and quantify all three of these characteristics so that different methods of \gls{UQ} can be compared objectively.}\label{fig:overview}
\end{figure}


%%%%%%%%%%%%%%%%%%%% Methods %%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Data handling}

All regressions in this paper were performed on a dataset of \gls{DFT} calculated adsorption energies created with \gls{GASpy}\cite{Tran2018, Tran2018a}.
These data included energies from 21,269 different H adsorption sites; 1,594 N sites; 18,437 CO sites; 2,515 O sites; and 3,464 OH sites; totaling in 47,279 data points.
\gls{GASpy} performed all \gls{DFT} calculations using \gls{VASP}\cite{Kresse1993, Kresse1994, Kresse1996, Kresse1996a} version 5.4 implemented in \gls{ASE}\cite{HjorthLarsen2017}.
The \gls{rPBE} functionals\cite{Hammer1999} were used along with \gls{VASP}'s pseudopotentials, and no spin magnetism or dispersion corrections were used.
Bulk relaxations were performed with a $10\times10\times10$ k-point grid and a 500 \gls{eV} cutoff, and only isotropic relaxation were allowed during this bulk relaxation.
Slab relaxations were performed with k-point grids of $4\times4\times1$ and a 350 \gls{eV} cutoff.
Slabs were replicated in the X/Y directions so that each cell was at least 4.5 \AA{} wide, which reduces adsorbate self-interaction.
Slabs were also replicated in the Z direction until they were at least 7 \AA{} thick, and at least 20 \AA{} of vacuum was included in between slabs.
The bottom layers of each slab were fixed and defined as those atoms more than 3 \AA{} from the top of the surface in the scaled Z direction.

To split the data into train/validate/test sets, we enumerated all adsorption energies on monometallic slabs and added them to the training set manually. 
We did this because some of the regression methods in this paper use a featurization that contains our monometallic adsorption energy data\cite{Tran2018}, and so having the monometallic adsorption energies pre-allocated in the training set prevented any information leakage between the training set and validation/test sets.
After this allocation, we performed a 64/14/20 train/validate/test split that was stratified\cite{Thompson2012} by adsorbate.
We then used the validation set's results to tune various hyperparameters manually.
After tuning, we calculated the training set results and present them in this paper exclusively.
Note that the test results were obtained using models that were trained only using the training set, not the validation set.
This is acceptable because we only seek to compare methods here, not to optimize them.


\subsection{Regression methods}

We explore a number of predictive \gls{UQ} methods that aim to quantify the uncertainty for regression procedures, where the predicted quantity is a continuous variable.
In order to standardize the assessment of performance, we ensure that each \gls{UQ} method returns predictive uncertainty results in a consistent format:  a distribution over possible outcomes of the predicted quantity for any specified input point.
This result format allows us to compute all of the predictive uncertainty performance metrics which we will introduce in subsequent sections.
Figure~\ref{fig:methods} illustrates all of the methods we investigate in this study, and we describe each method in detail below.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{methods/methods.pdf}
    \caption{Overview of the various \gls{UQ} methods we investigated in this study.
    $\Delta E$ represents \gls{DFT}-calculated adsorption energies;
    $\Delta \hat{E}$ represents \gls{ML}-predicted adsorption energies;
    $UQ$ represents \gls{ML}-predicted uncertainty quantifications;
    $\mu$ represents the mean of a sample of points;
    $\sigma$ represents the standard deviation of a sample of points;
    $\epsilon$ represents the residuals between \gls{DFT} and \gls{ML};
    and $\hat{\epsilon}$ represents the residuals between \gls{ML}-predicted $\epsilon$ and the actual $\epsilon$.
    }\label{fig:methods}
\end{figure}

\textbf{\glsentrytext{NN}:}
To establish a baseline for predictive accuracy, we re-trained a previously reported \gls{NN}\cite{Xie2018, Back2019} on this study's training set.
This \gls{NN} model projects a three-dimensional atomic structure into a graph, which is then fed into a convolutional neural network to predict various properties.
In this case, we predict \gls{DFT}-calculated adsorption energies, $\Delta$E.
Reference \citet{Back2019} for additional details.

\textbf{\glsentrytext{NN} Ensemble:}
We created an ensemble of \gls{NN}s by K-fold subsampling the training data into five different folds and then training individual \gls{NN} models on each of these folds.
Thus the mean of each of these models' predictions is the final prediction of the ensemble, and the standard deviation of the individual predictions is the ensemble's estimate of uncertainty.
% TODO:  Willie to provide justification for subsampling method. And maybe a reference for K-folding?

\textbf{\glsentrytext{BNN}:}
The aim of \gls{BNN} is to determine the posterior distribution of model parameters rather than a single optimal value of the parameters.
In practice, inferring true posterior is very difficult and even infeasible in most cases.
Thus, we approximate the model posterior to be as close as possible to the true posterior by doing variational inference.
This could be done by training the \gls{BNN} to minimize the Kullback-Leibler divergence between the variational distribution and the true posterior.
We sampled the model parameters K times from the approximated posterior, and used the mean of these predictions as the final prediction and the standard deviation of these predictions as the estimation of uncertainty.
We implemented the \gls{BNN} and performed variational inference using Pyro.\cite{Bingham2018}

\textbf{\glsentrytext{dNN}:}
Suppose we have trained a \gls{NN}.
We may aim to empirically fit an additional mapping that predicts the error of the first \gls{NN}.
Here we show \gls{dNN}, which trains a secondary \gls{NN} to predict the residuals of the initial \gls{NN}.
When training the first \gls{NN}, we hold out 10\% of the training data.
Afterwards, we use the residuals of the initial \gls{NN} on the held-out portion as training data for the second \gls{NN}.
After the secondary training, this second \gls{NN} can predict residuals for the first \gls{NN} on some new set of input data.
The predictions of the second \gls{NN} can then be used as uncertainty estimates.

\textbf{\glsentrytext{GP}:}
\gls{GP}s are one of the most common regression methods for producing \gls{UQ}s, and so we use them here as a baseline.
We fit a standard \gls{GP} using the same exact features that we used in previous work.\cite{Tran2018}
These features are defined by the elements coordinated with the adsorbate and by the elements of its next-nearest neighbors.
Specifically:  We use the atomic numbers of these elements, their Pauling electronegativity, a count of the number of atoms of each element near the adsorbate, and the median adsorption energy between the adsorbate and the elements.
To ensure that these features interacted well with the \gls{GP}'s kernel, we normalized each of the features to have a mean of zero and standard deviation of one.
Reference \citet{Tran2018} for additional details.
To define the \gls{GP}, we assumed a constant mean and used a Matern covariance kernel.
We trained the length scale of the Matern kernel using the \gls{MLE} method.
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.

\textbf{\glsentrytext{dGP}:}
\gls{GP}s are Bayesian models in which a prior distribution is first specified and then updated given observations to yield a posterior distribution. 
The mean of this posterior distribution is used for regression, and the covariance matrix is used for \gls{UQ}. 
Typically, in lieu of any additional prior knowledge, practitioners will take the prior distribution to have zero-mean. 
However, we could instead supply an alternative curve for the prior mean, and then perform the usual Bayesian updates to compute the posterior of this \gls{GP} given observations. 
Here, for the \gls{GP} prior mean, we supply the prediction given by a single pre-trained \gls{NN}. 
We call this method \gls{dGP}.
For the \gls{GP}, we used a Matern covariance kernel, where we fit the kernel hyperparameters using \gls{MLE}. 
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.

\textbf{\glsentrytext{CFGP}:}
A limitation of using this formulation of a \gls{GP} with \gls{NN}-predicted mean is that it requires the use of hand-crafted features for the \gls{GP}.
This requirement reduces the transferability of the method to other applications where such features may not be readily available.
To address this, we formulated a different method whereby we first train a neural network on the learning task (i.e., predict adsorption energies), and then we use the pooled outputs of the convolutional layers of the network as features in a new \gls{GP}.
The \gls{GP} would then be trained to use these features to produce both mean and uncertainty predictions on the adsorption energies.
We call this a \gls{CFGP}.
In this case, we used the baseline \gls{NN} as the network from which we obtained the convolution outputs.
We also normalized the convolution outputs of the \gls{NN} so that each output would have a mean of zero and a standard deviation of one.
To define the \gls{GP}, we assumed a constant mean and used a Matern covariance kernel.
We trained the length scale of the Matern kernel using the \gls{MLE} method.
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.


\subsection{Performance metrics}

We used six different metrics to quantify the accuracy of the various models:  \gls{MDAE}, \gls{RMSE}, \gls{MAE}, \gls{MARPD}, and \gls{R2}.
We used \gls{MDAE} because is insensitive to outliers and is therefore a good measure of accuracy for the majority of the data.
We used \gls{RMSE} because it is sensitive to outliers and is therefore a good measure of worst-case accuracy.
We used \gls{MAE} because it lies between \gls{MDAE} and \gls{RMSE} in terms of sensitivity to outliers.
We used \gls{MARPD} and \gls{R2} because they provide normalized measures of accuracy that may be more interpretable for those unfamiliar with adsorption energy measurements in \gls{eV}.
\gls{MARPD} values were calculated with Equation~\ref{eq:marpd} where $n$ is the index of a data point, $N$ is the total number of data points, $x_n$ is the true value of the data point, and $\hat{x}_n$ is the model's estimate of $x_n$.
In this case, $x_n$ is a DFT-calculated adsorption energy and $\hat{x}_n$ is the surrogate-model-calculated adsorption energy.
The ensemble of these metrics provide a more robust view of accuracy than any one metric can provide alone.

\begin{equation}\label{eq:marpd}
    MARPD = \frac{1}{N} \sum_{n=1}^{N} \abs{100 \cdot \frac{\hat{x}_n - x_n}{\abs{\hat{x}_n} + \abs{x_n}}}
\end{equation}

To assess the calibration (or ``honesty'') of these models' UQs, we created calibration curves. A calibration curve ``displays the true frequency of points in each [prediction] interval relative to the predicted fraction of points in that interval'', as outlined by \citet{Kuleshov2018}.
In other words:  We used the standard deviation predictions to create Gaussian-shaped prediction intervals around each test point, and then we compared these intervals to the models' residuals at these points.
If the residuals tended to fall outside the prediction intervals too often, then the UQs were considered overconfident.
If the residuals tended to fall inside the prediction intervals too often, then the UQs were considered underconfident.
Thus ``well-calibrated'' models had residuals that created a Gaussian distribution whose standard deviation was close to the model's predicted standard deviations.
We discuss calibration curves in more detail in the Results section alongside specific examples.

As \citet{Kuleshov2018} also pointed out, well-calibrated models are necessary but not sufficient for useful UQs.
For example:  A well-calibrated model could still have large uncertainty estimates, which are inherently less useful than well-calibrated and small uncertainty estimates.
This idea of having small uncertainty estimates is called ``sharpness'', and \citet{Kuleshov2018} define it with Equation~\ref{eq:og_sharpness}

\begin{equation}\label{eq:og_sharpness}
    sha = \frac{1}{N} \sum_{n=1}^{N} var(F_n)
\end{equation}

\noindent where $var(F_n)$ is the variance of the cumulative distribution function $F$ at point $n$.
This is akin to the average variance of the uncertainty estimates on the test set.
Here we propose and use a new formulation (Equation~\ref{eq:sharpness}) where we add a square root operation.
This operation gives the sharpness the same units as the predictions, which provides us with a more intuitive reference.
In other words:  Sharpness is akin to the average of the \gls{ML}-predicted standard deviations.

\begin{equation}\label{eq:sharpness}
    sha = \sqrt{\frac{1}{N} \sum_{n=1}^{N} var(F_n)}
\end{equation}

Finally, we assess the performance of each predictive uncertainty method via a the \gls{NLL} on a held out set of data.
This metric provides an overall assessment that is influenced by both the predictive accuracy of a method as well as the quality of its uncertainty quantification.
Previous work~\cite{} has shown the \gls{NLL} to be a proper scoring rule, which intuitively means that it provides a fair quantitative assessment of the uncertainty quantification and that it can be decomposed into terms that relate to both calibration and sharpness.
% TODO:  [Willie] Find a citation for this
We include results for \gls{NLL} since it is a popular performance metric that has been used to quantify uncertainty in a variety of prior work~\cite{} and provides an additional single score for uncertainty quantification methods.
% TODO:  [Willie] Find a citation for this
Let $\ell_n$ denote the natural logarithm of the predictive probability density function at a given held-out point $n$.
We then define \gls{NLL} using Equation~\ref{eq:nll}.
In general, a lower \gls{NLL} value indicates a better fit.

\begin{equation}\label{eq:nll}
  NLL = - \sum_{n=1}^{N} \ell_n
\end{equation}


%%%%%%%%%%%%%%%%%%%% Results %%%%%%%%%%%%%%%%%%%%

\section{Results}

\subsection{Illustrative examples}

Let us first discuss the results of our \gls{NN} ensemble for illustrative purposes.
Figure~\ref{fig:results_example} contains a parity plot, calibration curve, and predicted-uncertainty distribution of our \gls{NN} ensemble model.
The parity plot shows the accuracy of the model; the calibration curve shows the honesty of the model's uncertainty predictions; and the uncertainty distribution shows the sharpness of model's uncertainty predictions.
Accurate models have parity plots whose points tend to fall near the diagonal parity line.
Calibrated models have calibration curves that approach the ideal diganoal line.
Sharp models have uncertainty distributions that tend towards zero.
Note that sharpness should not be won at the cost of calibration.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/parity.pdf}
        \caption{parity plot}\label{fig:parity_example}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{calibration curve}\label{fig:calibration_example}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/sharpness.pdf}
        \caption{uncertainty distribution}\label{fig:sharpness_example}
    \end{subfigure}
    \caption{Results of the \gls{NN} ensemble. Each figure here was created with the test set of 8,289 points.}\label{fig:results_example}
\end{figure}

The calibration curve was created by first normalizing all test residuals by their respective uncertainty values, i.e.\ we divided the residuals by their predicted standard deviations.
If we assume that the normalized residuals follow a Gaussian distribution, then 68\% of the normalized residuals would fall between [-1, 1], 95\% of them would fall between [-2, 2], 99\% of them would fall between [-3, 3], etc.
To challenge this assumption, we plotted the actual fraction of points within each prediction interval against the expected fraction of points.
This plot is a calibration curve.
Thus a perfectly calibrated model would have normalized residuals that are perfectly Gaussian, which would yield a diagonal calibration line.
Therefore, models' calibration could be qualified by the closeness of their calibration curves to this ideal, diagonal curve.
We quantified this closeness by calculating the area between the calibration curve and the ideal diagonal.
We call this the miscalibration area, and smaller values indicate better calibration.

The shape of a calibration curve could also yield other insights.
If a model's \gls{UQ}s were too low/confident, then the normalized residuals would be too large and they would fall outside their expected prediction intervals too frequently.
This would result in a lower fraction of points observed within the expected prediction intervals, which would correspond to a calibration curve that falls below the ideal diagonal.
Therefore, overconfident models yield calibration curves that fall under the ideal diagonal, and underconfident models yield calibration curves that fall over the ideal diagonal.
Figure~\ref{fig:error_bars} illustrates this point by plotting calibration curves of various models alongside their parity plots that contain error bars corresponding to $\pm$2 standard deviations.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{calibration of \gls{NN} ensemble}\label{fig:calibration_example_ensemble}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/calibration.pdf}
        \caption{calibration of \gls{GP}}\label{fig:calibration_example_gp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../CFGP/Matern/calibration.pdf}
        \caption{calibration of \gls{CFGP}}\label{fig:calibration_example_cfgp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/error_bar_parity.pdf}
        \caption{parity of \gls{NN} ensemble}\label{fig:error_bar_ensemble}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/error_bar_parity.pdf}
        \caption{parity of \gls{GP}}\label{fig:error_bar_gp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../CFGP/Matern/error_bar_parity.pdf}
        \caption{parity of \gls{CFGP}}\label{fig:error_bar_cfgp}
    \end{subfigure}
    \caption{Calibration curves and parity plots of an overconfident \gls{NN} ensemble, an underconfident \gls{GP}, and better-calibrated \gls{CFGP}.
    The vertical uncertainty bands in the parity plots indicate $\pm$2 standard deviations in the uncertainty predictions of each model.
    For clarity, we sampled only 20 points of the 8,289 test points to put in the parity plots.
    It follows that relatively overconfident models would have more points with uncertainty bands that do not cross the diagonal parity line;
    relatively underconfident models would have more points that cross the diagonal parity line;
    and a well-calibrated model would have \textit{ca.} 19 out of 20 points cross the parity line.}\label{fig:error_bars}
\end{figure}

\subsection{Summary results}

Figure~\ref{fig:parity} contains parity plots for all \gls{UQ} methods studied here; Figure~\ref{fig:calibration} contains all calibration curves; and Figure~\ref{fig:sharpness} contains all distribution plots of the \gls{ML}-predicted \gls{UQ}s.
These figures illustrate the accuracy, calibration, and sharpness of the different \gls{UQ} methods, respectively.
Table~\ref{tab:results} lists their performance metrics.

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/parity.pdf}
        \caption{\gls{NN} ensemble}\label{fig:parity_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/parity.pdf}
        \caption{\gls{BNN}}\label{fig:parity_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/parity.pdf}
        \caption{\gls{dNN}}\label{fig:parity_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/parity.pdf}
        \caption{\gls{GP}}\label{fig:parity_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/parity.pdf}
        \caption{\gls{dGP}}\label{fig:parity_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../CFGP/Matern/parity.pdf}
        \caption{\gls{CFGP}}\label{fig:parity_cfgp}
    \end{subfigure}
    \caption{Parity plots for all \gls{UQ} methods used in this study.
    Shading plots were used in lieu of scatter plots because the large number of test points (8,289) obfuscated patterns.
    Darker shading indicates a higher density of points.
    Logarithmically scaled shading was used to accentuate outliers.
    The dashed, diagonal lines indicate parity.}\label{fig:parity}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{\gls{NN} ensemble}\label{fig:calibration_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/calibration.pdf}
        \caption{\gls{BNN}}\label{fig:calibration_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/calibration.pdf}
        \caption{\gls{dNN}}\label{fig:calibration_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/calibration.pdf}
        \caption{\gls{GP}}\label{fig:calibration_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/calibration.pdf}
        \caption{\gls{dGP}}\label{fig:calibration_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../CFGP/Matern/calibration.pdf}
        \caption{\gls{CFGP}}\label{fig:calibration_cfgp}
    \end{subfigure}
    \caption{Calibration curves for all \gls{UQ} methods used in this study.
    Dashed, blue lines indicate perfect calibration while solid orange lines indicate the experimental calibration of the test set.
    The blue, shaded area between these lines is defined as the miscalibration area.
    }\label{fig:calibration}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/sharpness.pdf}
        \caption{\gls{NN} ensemble}\label{fig:sharpness_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/sharpness.pdf}
        \caption{\gls{BNN}}\label{fig:sharpness_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/sharpness.pdf}
        \caption{\gls{dNN}}\label{fig:sharpness_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/sharpness.pdf}
        \caption{\gls{GP}}\label{fig:sharpness_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/sharpness.pdf}
        \caption{\gls{dGP}}\label{fig:sharpness_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../CFGP/Matern/sharpness.pdf}
        \caption{\gls{CFGP}}\label{fig:sharpness_cfgp}
    \end{subfigure}
    \caption{Distribution plots of the \gls{ML}-predicted standard deviations for each method.
    Sharpness values are indicated by vertical lines.}\label{fig:sharpness}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{lrrrrrrrr}
        \toprule
        Method              & \gls{MDAE}    & \gls{MAE} & \gls{RMSE}    & \gls{MARPD}   & \gls{R2}  & MisCal    & Sha   & \gls{NLL} [$\cdot$10\textsuperscript{3}] \\
        \midrule
        \gls{NN}            & 0.11          & 0.19      & 0.34          & 61            & 0.80      & N/A       & N/A   & N/A \\
        \gls{NN} ensemble   & 0.11          & 0.18      & 0.32          & 59            & 0.82      & 0.23      & 0.14  & 192.08 \\
        \gls{BNN}           & 0.11          & 0.19      & 0.31          & 59            & 0.83      & 0.41      & 0.03  & 669.61 \\
        \gls{dNN}           & 0.11          & 0.19      & 0.34          & 59            & 0.80      & 0.10      & 0.16  & 18.61 \\
        \gls{GP}            & 0.11          & 0.21      & 0.39          & 61            & 0.73      & 0.28      & 0.65  & 6.41 \\
        \gls{dGP}           & 0.11          & 0.19      & 0.33          & 59            & 0.81      & 0.05      & 0.21  & 6.09 \\
        \gls{CFGP}          & 0.11          & 0.19      & 0.33          & 59            & 0.80      & 0.06      & 0.24  & 2.80 \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics for all methods used in this study, which include:
    \glsentryfull{MDAE}, \glsentryfull{MAE}, \glsentryfull{RMSE}, \glsentryfull{MARPD},
    \glsentryfull{R2}, miscalibration area (MisCal), sharpness (Sha), and \glsentryfull{NLL}.
    The units of \gls{MDAE}, \gls{MAE}, \gls{RMSE}, and sharpness are all in \gls{eV}.
    The units of \gls{MARPD} are in \%.
    The miscalibration area and \gls{NLL} are unitless.}\label{tab:results}
\end{table}

Regarding accuracy:  All methods' \gls{MDAE} results are virtually identical, and their \gls{MAE} results are within 10\% of each other.
This suggests that all methods have comparable predictive accuracies for inliers.
The plain \gls{GP} has a higher \gls{RMSE} value than the rest of the methods though.
Thus our results suggest that our \gls{NN}-based methods are more accurate at outlier prediction than our sole non-\gls{NN}-based method.
Each of our \gls{NN}-based methods may not have practically different accuracies between each other though.

Regarding calibration:  The \gls{NN} ensemble and \gls{BNN} are overconfident; the \gls{GP} is underconfident; and the \gls{dNN}, \gls{dGP}, and \gls{CFGP} models are relatively calibrated.
The three more calibrated methods all share a characteristic that the other methods do not:  They all start with a \gls{NN} that is dedicated for prediction alone, and then they end with some other in-series method to estimate uncertainty.
Interestingly, this in-series method of learning predictions and then learning uncertainties is similar in spirit to how deep networks ``learn in stages'' at each subseqent hidden layer.

Regarding sharpness:  The \gls{NN} ensemble and \gls{BNN} yield the sharpest uncertainties, although both do so at the cost of calibration.
Among the three more calibrated models, the \gls{dNN} yields the lowest sharpness of 0.16 eV while the \gls{dGP} and \gls{CFGP} yield sharpnesses of 0.21 and 0.24 eV, respectively.
Note how \gls{GP}-based \gls{UQ} methods tend to yield less sharp uncertainties than methods based purely on \gls{NN}s.
This suggests that \gls{GP}s may yield more conservative \gls{UQ}s.

Regarding \gls{NLL}:  The \gls{CFGP} method yields the best (i.e., lowest) \gls{NLL} value of \textit{ca.} 2,800 while both the \gls{GP} and \gls{dGP} models yield relatively moderate \gls{NLL} values of \textit{ca.} 6,000.
Note how the under-confident \gls{GP} model has a worse miscalibration area and sharpness than the \gls{dNN} but a better \gls{NLL} value.
Simultaneously, the two most over-confident models (\gls{NN} ensemble and \gls{BNN}) yield the worst \gls{NLL} results.
This shows that better \gls{NLL} values correlate with relatively conservative estimates of \gls{UQ}, but not with relatively liberal estimates.
In other words:  If we use \gls{NLL} as our main performance metric, then we will favor under-confident \gls{UQ} estimates in lieu of over-confident estimates.

Given the performance metrics for accuracy, calibration, sharpness, and \gls{NLL}, we expect the \gls{CFGP} method to yield the best performing \gls{UQ} model for our dataset.
It yields the best \gls{NLL} while maintaining a relatively competitive accuracy and calibration with a moderate sharpness.

When choosing \gls{UQ} methods for different applications though, other factors should be considered.
For example:  Although the \gls{dGP} method performed relatively well, it required the use of a hand-crafted set of features.
If future researchers wish to use the \gls{dGP} method to predict other properties from atomic structures, they may have to define their own set of features.
This process of feature engineering is non-trivial and varies from application to application.
In some cases, it may be easier to use a \gls{UQ} method that does not require any additional features beyond the \gls{NN} input, such as \gls{dNN} or \gls{CFGP}.

Another factor to consider is the overhead cost of implementation.
For example:  The \gls{NN} ensemble method is arguably the simplest \gls{NN}-based \gls{UQ} method used here and may be the easiest or fastest method to implement.
Conversely, \gls{NN} ensembles also have a higher computational training cost than some of the other methods used here, such as \gls{dNN} or \gls{CFGP}.
This high training cost is exacerbated if the ensemble is meant to be used in an active framework where the model needs to be trained continuously.
As another example:  The \gls{BNN} method yielded perhaps the worst results of all the methods studied here.
It could be argued that further optimization of the \gls{BNN} could have resulted in higher performance.
But creation and training of \gls{BNN}s is still an active area of research with less literature and support than non-Bayesian \gls{NN}s or \gls{GP}s.
This lack of support led to us spending nearly twice as long creating a \gls{BNN} compared to the other methods.
It follows that further optimization of the \gls{BNN} would be non-trivial and may not be worth the overhead investment.


%%%%%%%%%%%%%%%%%%%% Conclusions %%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

We outlined a procedure for comparing different methods for \glsentryfull{UQ}.
This procedure considers the accuracy of each method, the honesty of their uncertainty estimates (i.e., their calibration), and the size of their uncertainty estimates (i.e., their sharpness).
To assess accuracy, we outlined a common set of error metrics such as \gls{MAE} or \gls{RMSE}, among others.
To assess calibration, we showed how to create, interpret, and quantify calibration curves.
To assess sharpness, we showed how to calculate and plot sharpness.
To assess all three aspects simultaneously, we suggest using the \glsentryfull{NLL} as a performance metric.
The ensemble of all these metrics and figures can then be used to judge the relative performance of various UQ methods in a holistic fashion.

As a case sutdy, we tested six different methods for predicting \glsentryfull{DFT} calculated adsorption energies with \gls{UQ}.
The best performing method was a \glsentryfull{CFGP}, which used a pre-trained convolutional output from a \gls{NN} as features for a subsequent \gls{GP} to make uncertanity estimates.
Our studies also showed that the \gls{GP}-based methods we tested tended to yield higher and more conservative uncertainty estimates than the methods that used only \gls{NN}s and \gls{NN} derivatives.
We also found that in-series methods tended to yield more calibrated models---i.e., methods that use one model to make value predictions and then a subsequent model to make uncertainty estimates were more calibrated than models that attempted to make value and uncertainty predictions simultaneously.
These results are limited to our dataset though.
Results may vary for studies with different applications, different models, or different hyperparameters.
But the underpinning procedure we used to compare each of these models is still broadly applicable.

Note that it would be possible to recalibrate\cite{Kuleshov2018} each of the models in this study to improve their uncertainty estimates.
We purposefully ommitted recalibration in this study to (1) simplify the illustration of the \gls{UQ} assessment procedure; (2) assess the innate performance of each of these UQ methods without confounding with recalibration methods; and (3) reduce overhead investment.
Future work should consider recalibration if the feasible UQ methods provide insufficiently calibrated uncertainty predictions


%%%%%%%%%%%%%%%%%%%% Misc %%%%%%%%%%%%%%%%%%%%

\section*{Code availability} Visit \texttt{https://github.com/ulissigroup/uncertainty\_benchmarking} for the code used to create the results discussed in this paper.
The code dependencies are listed inside the repository.

\section*{Author information} Corresponding author email:  zulissi@andrew.cmu.edu.
The authors declare no competing financial interest.

\section*{Acknowledgements} This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. % chktex 8
We also acknowledge the exceptional documentation and clear examples in the GPyTorch\cite{Gardner2018} repository, which formed the basis on much of the \gls{GP} code used for this work.


%%%%%%%%%%%%%%%%%%%% Bibliography %%%%%%%%%%%%%%%%%%%%

\clearpage
\bibliography{uncertainty_benchmarking}

\end{document}
