\documentclass[]{achemso}

\usepackage{amsmath}        % Equation editing using flags of \begin{align} and \end{align}
\usepackage{commath}        % We have this here to use the \abs{} equation function
\usepackage{graphicx}       % Display figures using \includegraphics
\graphicspath{{figures/}}   % Location for figures relative to .tex file path
\usepackage{subcaption}     % Enable the \subfigure command for multi-paneled figures
\usepackage{booktabs}       % Use table things like \toprule or \bottomrule
\usepackage[table]{xcolor}  % Lets us use \rowcolors to make alternate table shading
\usepackage{makecell}       % Allow the use of the \makecell command that lets linebreaks in table cells
\usepackage{etoolbox}       % Make the bibliograph unjustified (to work around hbox errors)
\apptocmd{\thebibliography}{\raggedright}{}{}
\usepackage{glossaries}     % Enable the \newacronym and \gls commands to reference terms

\newacronym{DFT}{DFT}{Density Functional Theory}
\newacronym{VASP}{VASP}{the Vienna Ab-initio Simulation Package}
\newacronym{GASpy}{GASpy}{the Generalized Adsorption Simulator for Python}
\newacronym{rPBE}{rPBE}{revised Perdew-Burke-Ernzerhof}
\newacronym{ASE}{ASE}{the Atomic Simulation Environment}
\newacronym{NEB}{NEB}{nudged elastic band}
\newacronym{GP}{GP}{Gaussian Process}
\newacronym{UQ}{UQ}{uncertainty quantification}
\newacronym{MAE}{MAE}{Mean Absolute Error}
\newacronym{RMSE}{RMSE}{Root Mean Squared Error}
\newacronym{MDAE}{MDAE}{Median Absolute Error}
\newacronym{MARPD}{MARPD}{Mean Absolute Relative Percent Difference}
\newacronym{R2}{R\textsuperscript{2}}{R\textsuperscript{2} correlation coefficient}
\newacronym{Pearson}{PPMCC}{Pearson Product-Moment Correlation coefficent}
\newacronym{NN}{NN}{crystal graph convolutional Neural Network}
\newacronym{BNN}{BNN}{Bayesian Neural Network}
\newacronym{MLE}{MLE}{Maximum Likelihood Estimation}
\newacronym{PFGP}{PFGP}{Penultimate-Fed Gaussian Process}
\newacronym{dNN}{NN$\Delta$NN}{in-tandem NNs}
\newacronym{dGP}{NN$\Delta$GP}{in-tandem NN \& GP}
\newacronym{ML}{ML}{Machine Learning}
\newacronym{eV}{eV}{electron volts}


%%%%%%%%%%%%%%%%%%%% Title/Abstract %%%%%%%%%%%%%%%%%%%%
\title{Uncertainty quantification in deep networks for material property predictions}
\author{Kevin Tran}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\altaffiliation{These authors contributed equally to this work}
\author{Willie Neiswanger}
\affiliation{Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\altaffiliation{These authors contributed equally to this work}
\author{Junwoong Yoon}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\author{Eric Xing}
\affiliation{Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\author{Zachary W. Ulissi}
\affiliation{Chemical Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15217}
\email{zulissi@andrew.cmu.edu}

\begin{document}

%\setlength{\fboxrule}{0 pt}
%\begin{tocentry}
%    \includegraphics[width=\textwidth]{TOC/TOC.pdf}
%    TOC caption here.
%\end{tocentry}

\begin{abstract}
    Abstract here.
\end{abstract}


%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The fields of catalysis and materials science are burgeoning with methods to screen, design, and understand materials.\cite{Medford2018, Gu2019, Schleder2019, Alberi2019}
This research has spurned the creation of \gls{ML} models to predict various material properties.
Unfortunately, the design spaces for these models are sometimes too large and intractable to sample completely.
These undersampling issues can limit the training data and therefore the predictive power of the models.
It would be helpful to have an \gls{UQ} for a model so that we know when to trust the predictions and when not to.
More specifically:  \gls{UQ} would enable various online, active frameworks for materials discovery and design (e.g., active learning,\cite{Settles2012} online active learning,\cite{Chu2011} Bayesian optimization,\cite{Frazier2018} active search,\cite{Garnett2012} or goal oriented design of experiments\cite{Kandasamy}).

Such active frameworks have already been used successfully in the field of catalysis and materials informatics.
For example:  \citet{Peterson2016} has used a neural network to perform online active learning of \gls{NEB} calculations, reducing the number of force calls by an order of magnitude.
\citet{Torres2018} have also used online active learning to accelerate \gls{NEB} calculations, but they used a \gls{GP} model instead of a neural network.
\citet{Jinnouchi2019} have used online active learning to accelerate molecular dynamics simulations.
Each of these active methods are underpinned by models with \gls{UQ}, which has garnered increasing attention itself.\cite{Peterson2017, Musil2019}

% TODO: Willie to make overview of predictive uncertainty

To our knowledge though, we have not seen many comparisons of different methods for \gls{UQ} within the field of catalysis and materials informatics.
Here we attempt to resolve this issue by benchmarking different methods for \gls{UQ} (Figure~\ref{fig:overview}).
We acknowledge that there will not be one optimal method across all use cases, but we still find value in sharing these results so that others can build intuition from our results.
Perhaps more importantly, we have also establish a protocol for comparing the performance of different modeling and \gls{UQ} methods.

% TODO:  Kevin to make this figure
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{placeholder.png}
    \caption{Placeholder for overview of the paper}\label{fig:overview}
\end{figure}


%%%%%%%%%%%%%%%%%%%% Methods %%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Data handling}

All regressions in this paper were performed on a dataset of \gls{DFT} calculated adsorption energies created with \gls{GASpy}\cite{Tran2018, Tran2018a}.
These data included energies from 21,269 different H adsorption sites; 1,594 N sites; 18,437 CO sites; 2,515 O sites; and 3,464 OH sites; totaling in 47,279 data points.
\gls{GASpy} performed all \gls{DFT} calculations using \gls{VASP}\cite{Kresse1993, Kresse1994, Kresse1996, Kresse1996a} version 5.4 implemented in \gls{ASE}\cite{HjorthLarsen2017}.
The \gls{rPBE} functionals\cite{Hammer1999} were used along with \gls{VASP}'s pseudopotentials, and no spin magnetism or dispersion corrections were used.
Bulk relaxations were performed with a $10\times10\times10$ k-point grid and a 500 \gls{eV} cutoff, and only isotropic relaxation were allowed during this bulk relaxation.
Slab relaxations were performed with k-point grids of $4\times4\times1$ and a 350 \gls{eV} cutoff.
Slabs were replicated in the X/Y directions so that each cell was at least 4.5 \AA{} wide, which reduces adsorbate self-interaction.
Slabs were also replicated in the Z direction until they were at least 7 \AA{} thick, and at least 20 \AA{} of vacuum was included in between slabs.
The bottom layers of each slab were fixed and defined as those atoms more than 3 \AA{} from the top of the surface in the scaled Z direction.

To split the data into train/validate/test sets, we enumerated all adsorption energies on monometallic slabs and added them to the training set manually. 
We did this because some of the regression methods in this paper use a featurization that contains our monometallic adsorption energy data\cite{Tran2018}, and so having the monometallic adsorption energies pre-allocated in the training set prevented any information leakage between the training set and validation/test sets.
After this allocation, we performed a 64/14/20 train/validate/test split that was stratified\cite{Thompson2012} by adsorbate.
We then used the validation set's results to tune various hyperparameters manually.
After tuning, we calculated the training set results and present them in this paper exclusively.
Note that the test results were obtained using models that were trained only using the training set, not the validation set.
This is acceptable because we only seek to compare methods here, not to optimize them.

\subsection{Regression methods}

% TODO:  Willie to make intro to predictive uncertainty methods.
Figure~\ref{fig:methods} illustrates all the methods we investigated in this study.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{methods.pdf}
    \caption{Overview of the various \gls{UQ} methods we investigated in this study.
    $\Delta E$ represents \gls{DFT}-calculated adsorption energies;
    $\Delta \hat{E}$ represents \gls{ML}-predicted adsorption energies;
    $UQ$ represents \gls{ML}-predicted uncertainty quantifications;
    $\mu$ represents the mean of a sample of points;
    $\sigma$ represents the standard deviation of a sample of points;
    $\epsilon$ represents the residuals between \gls{DFT} and \gls{ML};
    and $\hat{\epsilon}$ represents the residuals between \gls{ML}-predicted $\epsilon$ and the actual $\epsilon$.
    }\label{fig:methods}
\end{figure}

\textbf{\glsentrytext{NN}:}
To establish a baseline for predictive accuracy, we re-trained a previously reported \gls{NN}\cite{Xie2018, Back2019} on this study's training set.
This \gls{NN} model projects a three-dimensional atomic structure into a graph, which is then fed into a convolutional neural network to predict various properties.
In this case, we predict \gls{DFT}-calculated adsorption energies, $\Delta$E.
Reference \citet{Back2019} for additional details.

\textbf{\glsentrytext{NN} Ensemble:}
We created an ensemble of \gls{NN}s by K-fold subsampling the training data into five different folds and then training individual \gls{NN} models on each of these folds.
Thus the mean of each of these models' predictions is the final prediction of the ensemble, and the standard deviation of the individual predictions is the ensemble's estimate of uncertainty.
% TODO:  Willie to provide justification for subsampling method. And maybe a reference for K-folding?

\textbf{\glsentrytext{BNN}:}
The aim of \gls{BNN} is to determine the posterior distribution of model parameters rather than a single optimal value of the parameters.
In practice, inferring true posterior is very difficult and even infeasible in most cases.
Thus, we approximate the model posterior to be as close as possible to the true posterior by doing variational inference.
This could be done by training the \gls{BNN} to minimize the Kullback-Leibler divergence between the variational distribution and the true posterior.
We sampled the model parameters K times from the approximated posterior, and used the mean of these predictions as the final prediction and the standard deviation of these predictions as the estimation of uncertainty.
We implemented the \gls{BNN} and performed variational inference using Pyro.\cite{Bingham2018}

\textbf{\glsentrytext{dNN}:}
Something about \gls{dNN}
% TODO:  Willie

\textbf{\glsentrytext{GP}:}
\gls{GP}s are one of the most common regression methods for producing \gls{UQ}s, and so we use them here as a baseline.
We fit a standard \gls{GP} using the same exact features that we used in previous work.\cite{Tran2018}
These features are defined by the elements coordinated with the adsorbate and by the elements of its next-nearest neighbors.
Specifically:  We use the atomic numbers of these elements, their Pauling electronegativity, a count of the number of atoms of each element near the adsorbate, and the median adsorption energy between the adsorbate and the elements.
To ensure that these features interacted well with the \gls{GP}'s kernel, we normalized each of the features to have a mean of zero and standard deviation of one.
Reference \citet{Tran2018} for additional details.
To define the \gls{GP}, we assumed a constant mean and used a Matern covariance kernel.
We trained the length scale of the Matern kernel using the \gls{MLE} method with a Gaussian likelihood.
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.
% TODO:  Willie double-check the wording here. I think we use a Gaussian likelihood, but does make it no longer MLE? You know best.

\textbf{\glsentrytext{dGP}:}
\gls{GP}s are Bayesian models, in which a prior distribution is first specified, and then updated given observations, to yield a posterior distribution. 
The mean of this posterior distribution is used for regression, and the covariance matrix is used for \gls{UQ}. 
Typically, in lieu of any additional prior knowledge, practitioners will take the prior distribution to have zero-mean. 
However, we could instead supply an alternative curve for the prior mean, and then perform the usual Bayesian updates to compute the posterior of this \gls{GP} given observations. 
Here, for the \gls{GP} prior mean, we supply the prediction given by a single pre-trained \gls{NN}. 
We call this method \gls{dGP}.
For the \gls{GP}, we used a Matern covariance kernel, where we fit the kernel hyperparameters using \gls{MLE}. 
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.

\textbf{\glsentrytext{PFGP}:}
A limitation of using this formulation of a \gls{GP} with \gls{NN}-predicted mean is that it requires the use of hand-crafted features for the \gls{GP}.
This requirement reduces the transferability of the method to other applications where such features may not be readily available.
To address this, we formulated a different method whereby we first train a neural network on the learning task (i.e., predict adsorption energies), and then we use the outputs of the penultimate layer of the network as features in a new \gls{GP}.
The \gls{GP} would then be trained to use these features to produce both mean and uncertainty predictions on the adsorption energies.
We call this a \gls{PFGP}.
In this case, we used the baseline \gls{NN} as the network from which we obtained the penultimate outputs.
We also normalized the penultimate outputs of the \gls{NN} so that each output would have a mean of zero and a standard deviation of one.
To define the \gls{GP}, we assumed a constant mean and used a Matern covariance kernel.
We trained the length scale of the Matern kernel using the \gls{MLE} method with a Gaussian likelihood.
All \gls{GP} training and predictions were done with GPU acceleration as implemented in GPyTorch.\cite{Gardner2018}.
% TODO:  Willie double-check the wording here. I think we use a Gaussian likelihood, but does make it no longer MLE? You know best.

\subsection{Performance metrics}

We used six different metrics to quantify the accuracy of the various models:  \gls{MDAE}, \gls{RMSE}, \gls{MAE}, \gls{MARPD}, \gls{R2}, and \gls{Pearson}.
We used \gls{MDAE} because is insensitive to outliers and is therefore a good measure of accuracy for the majority of the data.
We used \gls{RMSE} because it is sensitive to outliers and is therefore a good measure of worst-case accuracy.
We used \gls{MAE} because it lies between \gls{MDAE} and \gls{RMSE} in terms of sensitivity to outliers.
We used \gls{MARPD}, \gls{R2}, and \gls{Pearson} because they provide normalized measures of accuracy that may be more interpretable for those unfamiliar with adsorption energy measurements in \gls{eV}.
\gls{MARPD} values were calculated with Equation~\ref{eq:marpd} where $n$ is the index of a data point, $N$ is the total number of data points, $x_n$ is the true value of the data point, and $\hat{x}_n$ is the model's estimate of $x_n$.
In this case, $x_n$ is a DFT-calculated adsorption energy and $\hat{x}_n$ is the surrogate-model-calculated adsorption energy.
The ensemble of these metrics provide a more robust view of accuracy than any one metric can provide alone.

\begin{equation}\label{eq:marpd}
    MARPD = \frac{1}{N} \sum_{i=1}^{N} \abs{100 \cdot \frac{\hat{x}_n - x_n}{\abs{\hat{x}_n} + \abs{x_n}}}
\end{equation}

To assess the calibration (or ``honesty'') of these models' UQs, we created calibration curves. A calibration curve ``displays the true frequency of points in each [prediction] interval relative to the predicted fraction of points in that interval'', as outlined by \citet{Kuleshov2018}.
In other words:  We used the standard deviation predictions to create Gaussian-shaped prediction intervals around each test point, and then we compared these intervals to the models' residuals at these points.
If the residuals tended to fall outside the prediction intervals too often, then the UQs were considered overconfident.
If the residuals tended to fall inside the prediction intervals too often, then the UQs were considered underconfident.
Thus ``well-calibrated'' models had residuals that created a Gaussian distribution whose standard deviation was close to the model's predicted standard deviations.
We discuss calibration curves in more detail in the Results section alongside specific examples.

As \citet{Kuleshov2018} also pointed out, well-calibrated models are necessary but not sufficient for useful UQs.
For example:  A well-calibrated model could still have large uncertainty estimates, which are inherently less useful than well-calibrated and small uncertainty estimates.
This idea of having small uncertainty estimates is called ``sharpness'', and \citet{Kuleshov2018} define it with Equation~\ref{eq:og_sharpness}

\begin{equation}\label{eq:og_sharpness}
    sha = \frac{1}{N} \sum_{n=1}^{N} var(F_n)
\end{equation}

\noindent where $var(F_n)$ is the variance of the cumulative distribution function $F$ at point $n$.
This is akin to the average variance of the uncertainty estimates on the test set.
Here we propose and use a new formulation (Equation~\ref{eq:sharpness}) where we add a square root operation.
This operation gives the sharpness the same units as the predictions, which provides us with a more intuitive reference.
In other words:  Sharpness is akin to the average of the \gls{ML}-predicted standard deviations.

\begin{equation}\label{eq:sharpness}
    sha = \sqrt{\frac{1}{N} \sum_{n=1}^{N} var(F_n)}
\end{equation}


%%%%%%%%%%%%%%%%%%%% Results %%%%%%%%%%%%%%%%%%%%

\section{Results}

\subsection{Illustrative examples}

Let us first discuss the results of our \gls{NN} ensemble for illustrative purposes.
Figure~\ref{fig:results_example} contains a parity plot, calibration curve, and predicted-uncertainty distribution of our \gls{NN} ensemble model.
The parity plot shows the accuracy of the model; the calibration curve shows the honesty of the model's uncertainty predictions; and the uncertainty distribution shows the sharpness of model's uncertainty predictions.
Accurate models have parity plots whose points tend to fall near the diagonal parity line.
Sharp models have uncertainty distributions that tend towards zero.
Note that sharpness should not be won at the cost of calibration.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/parity.pdf}
        \caption{parity plot}\label{fig:parity_example}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{calibration curve}\label{fig:calibration_example}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/sharpness.pdf}
        \caption{uncertainty distribution}\label{fig:sharpness_example}
    \end{subfigure}
    \caption{Results of the \gls{NN} ensemble. Each figure here was created with the test set of 8,289 points.}\label{fig:results_example}
\end{figure}

The calibration curve was created by first normalizing all test residuals by their respective uncertainty values, i.e.\ we divided the residuals by their predicted standard deviations.
If we assume that the normalized residuals follow a Gaussian distribution, then 68\% of the normalized residuals would fall between [-1, 1], 95\% of them would fall between [-2, 2], 99\% of them would fall between [-3, 3], etc.
To challenge this assumption, we plotted the actual fraction of points within each prediction interval against the expected fraction of points.
This plot is a calibration curve.
Thus a perfectly calibrated model would have normalized residuals that are perfectly Gaussian, which would yield a diagonal calibration line.
Therefore, models' calibration could be qualified by the closeness of their calibration curves to this ideal, diagonal curve.
We quantified this closeness by calculating the area between the calibration curve and the ideal diagonal.
We call this the miscalibration area, and smaller values indicate better calibration.

The shape of a calibration curve could also yield other insights.
If a model's \gls{UQ}s were too low/confident, then the normalized residuals would be too large and they would fall outside their expected prediction intervals too frequently.
This would result in observed prediction intervals that are lower than expected, which would correspond to a calibration curve that falls below the ideal diagonal.
Therefore, overconfident models yield calibration curves that fall under the ideal diagonal, and underconfident models yield calibration curves that fall over the ideal diagonal.
Figure~\ref{fig:error_bars} illustrates this point by plotting calibration curves of various models alongside their parity plots that contain error bars corresponding to $\pm$2 standard deviations.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{calibration of \gls{NN} ensemble}\label{fig:calibration_example_ensemble}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/calibration.pdf}
        \caption{calibration of \gls{GP}}\label{fig:calibration_example_gp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../PFGP/Matern/calibration.pdf}
        \caption{calibration of \gls{PFGP}}\label{fig:calibration_example_pfgp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/error_bar_parity.pdf}
        \caption{parity of \gls{NN} ensemble}\label{fig:error_bar_ensemble}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/error_bar_parity.pdf}
        \caption{parity of \gls{GP}}\label{fig:error_bar_gp}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{../PFGP/Matern/error_bar_parity.pdf}
        \caption{parity of \gls{PFGP}}\label{fig:error_bar_pfgp}
    \end{subfigure}
    \caption{Calibration curves and parity plots of an overconfident \gls{NN} ensemble, an underconfident \gls{GP}, and better-calibrated \gls{PFGP}.
    The vertical uncertainty bands in the parity plots indicate $\pm$2 standard deviations in the uncertainty predictions of each model.
    For clarity, we sampled only 20 points of the 8,289 test points to put in the parity plots.
    It follows that relatively overconfident models would have more points with uncertainty bands that do not cross the diagonal parity line;
    relatively underconfident models would have more points that cross the diagonal parity line;
    and a well-calibrated model would have 19 out of 20 points cross the parity line.}\label{fig:error_bars}
\end{figure}

\subsection{Summary results}

Figure~\ref{fig:parity} contains parity plots for all \gls{UQ} methods studied here; Figure~\ref{fig:calibration} contains all calibration curves; and Figure~\ref{fig:sharpness} contains all distribution plots of the \gls{ML}-predicted \gls{UQ}s.
These figures illustrate the accuracy, calibration, and sharpness of the different \gls{UQ} methods, respectively.
Table~\ref{tab:results} lists their performance metrics.

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/parity.pdf}
        \caption{\gls{NN} ensemble}\label{fig:parity_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/parity.pdf}
        \caption{\gls{BNN}}\label{fig:parity_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/parity.pdf}
        \caption{\gls{dNN}}\label{fig:parity_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/parity.pdf}
        \caption{\gls{GP}}\label{fig:parity_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/parity.pdf}
        \caption{\gls{dGP}}\label{fig:parity_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../PFGP/Matern/parity.pdf}
        \caption{\gls{PFGP}}\label{fig:parity_pfgp}
    \end{subfigure}
    \caption{Parity plots for all \gls{UQ} methods used in this study.
    Shading plots were used in lieu of scatter plots because the large number of test points (8,289) obfuscated patterns.
    Darker shading indicates a higher density of points.
    Logarithmically scaled shading was used to accentuate outliers.
    The dashed, diagonal lines indicate parity.}\label{fig:parity}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/calibration.pdf}
        \caption{\gls{NN} ensemble}\label{fig:calibration_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/calibration.pdf}
        \caption{\gls{BNN}}\label{fig:calibration_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/calibration.pdf}
        \caption{\gls{dNN}}\label{fig:calibration_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/calibration.pdf}
        \caption{\gls{GP}}\label{fig:calibration_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/calibration.pdf}
        \caption{\gls{dGP}}\label{fig:calibration_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../PFGP/Matern/calibration.pdf}
        \caption{\gls{PFGP}}\label{fig:calibration_pfgp}
    \end{subfigure}
    \caption{Calibration curves for all \gls{UQ} methods used in this study.
    Dashed, blue lines indicate perfect calibration while solid orange lines indicate the experimental calibration of the test set.
    The blue, shaded area between these lines is defined as the miscalibration area.
    }\label{fig:calibration}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NN_ensemble/sharpness.pdf}
        \caption{\gls{NN} ensemble}\label{fig:sharpness_ensemble}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../BNN/sharpness.pdf}
        \caption{\gls{BNN}}\label{fig:sharpness_bnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdNN/sharpness.pdf}
        \caption{\gls{dNN}}\label{fig:sharpness_dnn}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../GP/Matern/sharpness.pdf}
        \caption{\gls{GP}}\label{fig:sharpness_gp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../NNdGP/Matern/sharpness.pdf}
        \caption{\gls{dGP}}\label{fig:sharpness_dgp}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{../PFGP/Matern/sharpness.pdf}
        \caption{\gls{PFGP}}\label{fig:sharpness_pfgp}
    \end{subfigure}
    \caption{Distribution plots of the \gls{ML}-predicted standard deviations for each method.
    Sharpness values are indicated by vertical lines.}\label{fig:sharpness}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{lrrrrrrrr}
        \toprule
        Method              & \gls{MDAE}    & \gls{MAE} & \gls{RMSE}    & \gls{MARPD}   & \gls{R2}  & \gls{Pearson} & MisCal    & Sha \\
        \midrule
        \gls{NN}            & 0.11          & 0.19      & 0.34          & 61            & 0.80      & 0.90          & N/A       & N/A \\
        \gls{NN} ensemble   & 0.11          & 0.18      & 0.32          & 59            & 0.82      & 0.90          & 0.23      & 0.14 \\
        \gls{BNN}           & 0.11          & 0.19      & 0.31          & 59            & 0.83      & 0.91          & 0.41      & 0.03 \\
        \gls{dNN}           & 0.11          & 0.19      & 0.34          & 59            & 0.80      & 0.89          & 0.10      & 0.16 \\
        \gls{GP}            & 0.11          & 0.21      & 0.39          & 61            & 0.73      & 0.85          & 0.28      & 0.65 \\
        \gls{dGP}           & 0.11          & 0.19      & 0.33          & 59            & 0.81      & 0.90          & 0.05      & 0.21 \\
        \gls{PFGP}          & 0.11          & 0.20      & 0.34          & 61            & 0.80      & 0.89          & 0.07      & 0.38 \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics for all methods used in this study, which include:
    \glsentryfull{MDAE}, \glsentryfull{MAE}, \glsentryfull{RMSE}, \glsentryfull{MARPD},
    \glsentryfull{R2}, \glsentryfull{Pearson},
    miscalibration area (MisCal), and sharpness (Sha).
    The units of \gls{MDAE}, \gls{MAE}, \gls{RMSE}, and sharpness are all in \gls{eV}.
    The units of \gls{MARPD} are in \%.
    The remaining metrics are unitless.}\label{tab:results}
\end{table}

All methods' \gls{MDAE} results are virtually identical, and their \gls{MAE} results are within 10\% of each other.
This suggests that all methods have comparable predictive accuracies for inliers.
The plain \gls{GP} has a higher \gls{RMSE} value than the rest of the methods though, suggesting that \gls{GP}s may be less accurate for outliers than \gls{NN}-based methods.

\begin{enumerate}
    \item{Blocking results on best candidates (maybe put in SI)}
    \item{Cost of computing each method (if it’s there)}
    \item{Human overhead and difficulty}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%% Conclusions %%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

\begin{enumerate}
    \item{Relative accuracies}
    \item{Relative calibrations}
    \item{Relative sharpnesses}
    \item{Overhead}
    \item{Calibration}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%% Misc %%%%%%%%%%%%%%%%%%%%

\section*{Code availability} Visit \texttt{https://github.com/ulissigroup/uncertainty\_benchmarking} for the code used to create the results discussed in this paper.
The code dependencies are listed inside the repository.
Due to GitHub's constraints on file sizes, the raw data cache used in this work (\texttt{splits\_gasdb.pkl}) could not be put into the repository directly.
This file is available upon request.

\section*{Author information} Corresponding author email:  zulissi@andrew.cmu.edu.
The authors declare no competing financial interest.

\section*{Acknowledgements} This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. % chktex 8
We also acknowledge the exceptional documentation and clear examples in the GPyTorch\cite{Gardner2018} repository, which formed the basis on much of the \gls{GP} code used for this work.


%%%%%%%%%%%%%%%%%%%% Bibliography %%%%%%%%%%%%%%%%%%%%

\clearpage
\bibliography{uncertainty_benchmarking}

\end{document}
