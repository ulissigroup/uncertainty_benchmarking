{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch Regression Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate many of the design features of GPyTorch using the simplest example, training an RBF kernel Gaussian process on a simple function. We'll be modeling the function\n",
    "\n",
    "\\begin{align*}\n",
    "  y &= \\sin(2\\pi x) + \\epsilon \\\\\n",
    "  \\epsilon &\\sim \\mathcal{N}(0, 0.2)\n",
    "\\end{align*}\n",
    "\n",
    "with 11 training examples, and testing on 51 test examples.\n",
    "\n",
    "**Note:** this notebook is not necessarily intended to teach the mathematical background of Gaussian processes, but rather how to train a simple one and make predictions in GPyTorch. For a mathematical treatment, Chapter 2 of Gaussian Processes for Machine Learning provides a very thorough introduction to GP regression (this entire text is highly recommended): http://www.gaussianprocess.org/gpml/chapters/RW2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "\n",
    "In the next cell, we set up the training data for this example. We'll be using 11 regularly spaced points on [0,1] which we evaluate the function on and add Gaussian noise to get the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "The next cell demonstrates the most critical features of a user-defined Gaussian process model in GPyTorch. Building a GP model in GPyTorch is different in a number of ways.\n",
    "\n",
    "First in contrast to many existing GP packages, we do not provide full GP models for the user. Rather, we provide *the tools necessary to quickly construct one*. This is because we believe, analogous to building a neural network in standard PyTorch, it is important to have the flexibility to include whatever components are necessary. As can be seen in more complicated examples, like the [CIFAR10 Deep Kernel Learning](../08_Deep_Kernel_Learning/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.ipynb) example which combines deep learning and Gaussian processes, this allows the user great flexibility in designing custom models.\n",
    "\n",
    "For most GP regression models, you will need to construct the following GPyTorch objects:\n",
    "\n",
    "1. A **GP Model** (`gpytorch.models.ExactGP`) -  This handles most of the inference.\n",
    "1. A **Likelihood** (`gpytorch.likelihoods.GaussianLikelihood`) - This is the most common likelihood used for GP regression.\n",
    "1. A **Mean** - This defines the prior mean of the GP.\n",
    "  - If you don't know which mean to use, a `gpytorch.means.ConstantMean()` is a good place to start.\n",
    "1. A **Kernel** - This defines the prior covariance of the GP.\n",
    "  - If you don't know which kernel to use, a `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())` is a good place to start.\n",
    "1. A **MultivariateNormal** Distribution (`gpytorch.distributions.MultivariateNormal`) - This is the object used to represent multivariate normal distributions.\n",
    "  \n",
    "  \n",
    "### The GP Model\n",
    "  \n",
    "The components of a user built (Exact, i.e. non-variational) GP model in GPyTorch are, broadly speaking:\n",
    "\n",
    "1. An `__init__` method that takes the training data and a likelihood, and constructs whatever objects are necessary for the model's `forward` method. This will most commonly include things like a mean module and a kernel module.\n",
    "\n",
    "2. A `forward` method that takes in some $n \\times d$ data `x` and returns a `MultivariateNormal` with the *prior* mean and covariance evaluated at `x`. In other words, we return the vector $\\mu(x)$ and the $n \\times n$ matrix $K_{xx}$ representing the prior mean and covariance matrix of the GP. \n",
    "\n",
    "This specification leaves a large amount of flexibility when defining a model. For example, to compose two kernels via addition, you can either add the kernel modules directly:\n",
    "\n",
    "```python\n",
    "self.covar_module = ScaleKernel(RBFKernel() + WhiteNoiseKernel())\n",
    "```\n",
    "\n",
    "Or you can add the outputs of the kernel in the forward method:\n",
    "\n",
    "```python\n",
    "covar_x = self.rbf_kernel_module(x) + self.white_noise_module(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model modes\n",
    "\n",
    "Like most PyTorch modules, the `ExactGP` has a `.train()` and `.eval()` mode.\n",
    "- `.train()` mode is for optimizing model hyperameters.\n",
    "- `.eval()` mode is for computing predictions through the model posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "In the next cell, we handle using Type-II MLE to train the hyperparameters of the Gaussian process.\n",
    "\n",
    "The most obvious difference here compared to many other GP implementations is that, as in standard PyTorch, the core training loop is written by the user. In GPyTorch, we make use of the standard PyTorch optimizers as from `torch.optim`, and all trainable parameters of the model should be of type `torch.nn.Parameter`. Because GP models directly extend `torch.nn.Module`, calls to methods like `model.parameters()` or `model.named_parameters()` function as you might expect coming from PyTorch.\n",
    "\n",
    "In most cases, the boilerplate code below will work well. It has the same basic components as the standard PyTorch training loop:\n",
    "\n",
    "1. Zero all parameter gradients\n",
    "2. Call the model and compute the loss\n",
    "3. Call backward on the loss to fill in gradients\n",
    "4. Take a step on the optimizer\n",
    "\n",
    "However, defining custom training loops allows for greater flexibility. For example, it is easy to save the parameters at each step of training, or use different learning rates for different parameters (which may be useful in deep kernel learning for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.931   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.899   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.864   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.825   lengthscale: 0.555   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.782   lengthscale: 0.514   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.734   lengthscale: 0.475   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.683   lengthscale: 0.439   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.631   lengthscale: 0.404   noise: 0.402\n",
      "Iter 9/50 - Loss: 0.581   lengthscale: 0.371   noise: 0.369\n",
      "Iter 10/50 - Loss: 0.535   lengthscale: 0.342   noise: 0.339\n",
      "Iter 11/50 - Loss: 0.493   lengthscale: 0.315   noise: 0.310\n",
      "Iter 12/50 - Loss: 0.454   lengthscale: 0.291   noise: 0.284\n",
      "Iter 13/50 - Loss: 0.418   lengthscale: 0.272   noise: 0.259\n",
      "Iter 14/50 - Loss: 0.383   lengthscale: 0.256   noise: 0.237\n",
      "Iter 15/50 - Loss: 0.348   lengthscale: 0.243   noise: 0.216\n",
      "Iter 16/50 - Loss: 0.313   lengthscale: 0.233   noise: 0.196\n",
      "Iter 17/50 - Loss: 0.278   lengthscale: 0.226   noise: 0.179\n",
      "Iter 18/50 - Loss: 0.244   lengthscale: 0.221   noise: 0.162\n",
      "Iter 19/50 - Loss: 0.209   lengthscale: 0.218   noise: 0.147\n",
      "Iter 20/50 - Loss: 0.175   lengthscale: 0.217   noise: 0.134\n",
      "Iter 21/50 - Loss: 0.141   lengthscale: 0.217   noise: 0.122\n",
      "Iter 22/50 - Loss: 0.107   lengthscale: 0.220   noise: 0.110\n",
      "Iter 23/50 - Loss: 0.075   lengthscale: 0.223   noise: 0.100\n",
      "Iter 24/50 - Loss: 0.043   lengthscale: 0.228   noise: 0.091\n",
      "Iter 25/50 - Loss: 0.013   lengthscale: 0.234   noise: 0.083\n",
      "Iter 26/50 - Loss: -0.015   lengthscale: 0.241   noise: 0.075\n",
      "Iter 27/50 - Loss: -0.041   lengthscale: 0.250   noise: 0.069\n",
      "Iter 28/50 - Loss: -0.065   lengthscale: 0.259   noise: 0.063\n",
      "Iter 29/50 - Loss: -0.087   lengthscale: 0.269   noise: 0.057\n",
      "Iter 30/50 - Loss: -0.105   lengthscale: 0.279   noise: 0.052\n",
      "Iter 31/50 - Loss: -0.119   lengthscale: 0.289   noise: 0.048\n",
      "Iter 32/50 - Loss: -0.131   lengthscale: 0.299   noise: 0.044\n",
      "Iter 33/50 - Loss: -0.138   lengthscale: 0.308   noise: 0.041\n",
      "Iter 34/50 - Loss: -0.143   lengthscale: 0.315   noise: 0.038\n",
      "Iter 35/50 - Loss: -0.145   lengthscale: 0.320   noise: 0.035\n",
      "Iter 36/50 - Loss: -0.146   lengthscale: 0.322   noise: 0.033\n",
      "Iter 37/50 - Loss: -0.145   lengthscale: 0.321   noise: 0.031\n",
      "Iter 38/50 - Loss: -0.144   lengthscale: 0.318   noise: 0.030\n",
      "Iter 39/50 - Loss: -0.142   lengthscale: 0.312   noise: 0.028\n",
      "Iter 40/50 - Loss: -0.139   lengthscale: 0.306   noise: 0.027\n",
      "Iter 41/50 - Loss: -0.136   lengthscale: 0.300   noise: 0.026\n",
      "Iter 42/50 - Loss: -0.133   lengthscale: 0.294   noise: 0.026\n",
      "Iter 43/50 - Loss: -0.130   lengthscale: 0.288   noise: 0.025\n",
      "Iter 44/50 - Loss: -0.129   lengthscale: 0.283   noise: 0.025\n",
      "Iter 45/50 - Loss: -0.128   lengthscale: 0.280   noise: 0.025\n",
      "Iter 46/50 - Loss: -0.128   lengthscale: 0.278   noise: 0.025\n",
      "Iter 47/50 - Loss: -0.129   lengthscale: 0.276   noise: 0.025\n",
      "Iter 48/50 - Loss: -0.131   lengthscale: 0.277   noise: 0.025\n",
      "Iter 49/50 - Loss: -0.133   lengthscale: 0.278   noise: 0.026\n",
      "Iter 50/50 - Loss: -0.136   lengthscale: 0.280   noise: 0.026\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with the model\n",
    "\n",
    "In the next cell, we make predictions with the model. To do this, we simply put the model and likelihood in eval mode, and call both modules on the test data.\n",
    "\n",
    "Just as a user defined GP model returns a `MultivariateNormal` containing the prior mean and covariance from forward, a trained GP model in eval mode returns a `MultivariateNormal` containing the posterior mean and covariance. Thus, getting the predictive mean and variance, and then sampling functions from the GP at the given test points could be accomplished with calls like:\n",
    "\n",
    "```python\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n",
    "```\n",
    "\n",
    "The `gpytorch.settings.fast_pred_var` context is not needed, but here we are giving a preview of using one of our cool features, getting faster predictive distributions using [LOVE](https://arxiv.org/abs/1803.06058)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model fit\n",
    "\n",
    "In the next cell, we plot the mean and confidence region of the Gaussian process model. The `confidence_region` method is a helper method that returns 2 standard deviations above and below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    #ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as black line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'k')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, color='orange')\n",
    "    ax.set_ylim([-2, 2])\n",
    "    #ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.legend(['$\\Delta \\hat{E}$', 'UQ'])\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('$\\Delta$E')\n",
    "    \n",
    "    plt.savefig('GP.pdf', dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it all again, but with a zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.787   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.750   lengthscale: 0.744   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.713   lengthscale: 0.798   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.675   lengthscale: 0.854   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.637   lengthscale: 0.911   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.598   lengthscale: 0.971   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.559   lengthscale: 1.031   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.519   lengthscale: 1.093   noise: 0.403\n",
      "Iter 9/50 - Loss: 0.478   lengthscale: 1.157   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.437   lengthscale: 1.220   noise: 0.340\n",
      "Iter 11/50 - Loss: 0.396   lengthscale: 1.285   noise: 0.312\n",
      "Iter 12/50 - Loss: 0.354   lengthscale: 1.350   noise: 0.286\n",
      "Iter 13/50 - Loss: 0.312   lengthscale: 1.415   noise: 0.262\n",
      "Iter 14/50 - Loss: 0.270   lengthscale: 1.480   noise: 0.239\n",
      "Iter 15/50 - Loss: 0.228   lengthscale: 1.546   noise: 0.218\n",
      "Iter 16/50 - Loss: 0.185   lengthscale: 1.611   noise: 0.199\n",
      "Iter 17/50 - Loss: 0.143   lengthscale: 1.676   noise: 0.181\n",
      "Iter 18/50 - Loss: 0.100   lengthscale: 1.741   noise: 0.165\n",
      "Iter 19/50 - Loss: 0.058   lengthscale: 1.805   noise: 0.150\n",
      "Iter 20/50 - Loss: 0.016   lengthscale: 1.869   noise: 0.137\n",
      "Iter 21/50 - Loss: -0.026   lengthscale: 1.932   noise: 0.124\n",
      "Iter 22/50 - Loss: -0.068   lengthscale: 1.994   noise: 0.113\n",
      "Iter 23/50 - Loss: -0.108   lengthscale: 2.056   noise: 0.102\n",
      "Iter 24/50 - Loss: -0.148   lengthscale: 2.117   noise: 0.093\n",
      "Iter 25/50 - Loss: -0.188   lengthscale: 2.176   noise: 0.084\n",
      "Iter 26/50 - Loss: -0.226   lengthscale: 2.235   noise: 0.077\n",
      "Iter 27/50 - Loss: -0.263   lengthscale: 2.293   noise: 0.069\n",
      "Iter 28/50 - Loss: -0.299   lengthscale: 2.351   noise: 0.063\n",
      "Iter 29/50 - Loss: -0.334   lengthscale: 2.407   noise: 0.057\n",
      "Iter 30/50 - Loss: -0.366   lengthscale: 2.462   noise: 0.052\n",
      "Iter 31/50 - Loss: -0.398   lengthscale: 2.515   noise: 0.047\n",
      "Iter 32/50 - Loss: -0.427   lengthscale: 2.568   noise: 0.043\n",
      "Iter 33/50 - Loss: -0.454   lengthscale: 2.620   noise: 0.039\n",
      "Iter 34/50 - Loss: -0.479   lengthscale: 2.671   noise: 0.036\n",
      "Iter 35/50 - Loss: -0.501   lengthscale: 2.720   noise: 0.033\n",
      "Iter 36/50 - Loss: -0.521   lengthscale: 2.769   noise: 0.030\n",
      "Iter 37/50 - Loss: -0.539   lengthscale: 2.816   noise: 0.028\n",
      "Iter 38/50 - Loss: -0.553   lengthscale: 2.862   noise: 0.025\n",
      "Iter 39/50 - Loss: -0.565   lengthscale: 2.907   noise: 0.024\n",
      "Iter 40/50 - Loss: -0.575   lengthscale: 2.951   noise: 0.022\n",
      "Iter 41/50 - Loss: -0.582   lengthscale: 2.993   noise: 0.020\n",
      "Iter 42/50 - Loss: -0.586   lengthscale: 3.034   noise: 0.019\n",
      "Iter 43/50 - Loss: -0.589   lengthscale: 3.074   noise: 0.018\n",
      "Iter 44/50 - Loss: -0.590   lengthscale: 3.113   noise: 0.017\n",
      "Iter 45/50 - Loss: -0.589   lengthscale: 3.151   noise: 0.016\n",
      "Iter 46/50 - Loss: -0.587   lengthscale: 3.187   noise: 0.015\n",
      "Iter 47/50 - Loss: -0.585   lengthscale: 3.222   noise: 0.015\n",
      "Iter 48/50 - Loss: -0.582   lengthscale: 3.255   noise: 0.014\n",
      "Iter 49/50 - Loss: -0.579   lengthscale: 3.288   noise: 0.014\n",
      "Iter 50/50 - Loss: -0.577   lengthscale: 3.319   noise: 0.014\n"
     ]
    }
   ],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "# True function is just noise\n",
    "train_y = torch.sin(0 + torch.randn(train_x.size())) * 0.2\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    #ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as black line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'k')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5, color='orange')\n",
    "    ax.set_ylim([-1, 1])\n",
    "    #ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.legend(['$\\hat{\\epsilon}$', 'UQ'])\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('$\\epsilon$')\n",
    "    \n",
    "    plt.savefig('GP_flat.pdf', dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ktran",
   "language": "python",
   "name": "ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
