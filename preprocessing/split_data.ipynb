{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads the preprocessed data created by `./sdt/*/create_sdt.py` and `./fingerprint/fingerprint_*.ipynb` and then splits them into train, validate, and test sets.\n",
    "\n",
    "Note that our GASpy fingerprinter was \"trained\" on all of our data, but it used only the adsorption energy data on monometallic surfaces. To ensure the integrity of our validation and test metrics, we should put the data points from the monometallic surface structures in the training set explicitly. This way there is no information leak from our validation/test sets into our training sets. We do this allocation here.\n",
    "\n",
    "We also stratify our data by adsorbate. This means that we first split our data by adsorbate, and then we perform a separate train/validate/test split for subset of the data. We then concatenate the training, validation, and test sets back together. This ensures that each of our data partitions has a proportional amount of data from each adsorbate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/ktran/miniconda3/envs/gaspy/lib/python3.6/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/global/homes/k/ktran/miniconda3/envs/gaspy/lib/python3.6/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GASdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47279 documents/data points\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('pull_data/gaspy/docs.pkl', 'rb') as file_handle:\n",
    "    docs = pickle.load(file_handle)\n",
    "print('%i documents/data points' % len(docs))\n",
    "\n",
    "with open('sdt/gasdb/sdts.pkl', 'rb') as file_handle:\n",
    "    sdts = pickle.load(file_handle)\n",
    "\n",
    "with open('fingerprint/fingerprints_gasdb.pkl', 'rb') as file_handle:\n",
    "    fingerprints = pickle.load(file_handle)\n",
    "\n",
    "# Targets = adsorption energies\n",
    "targets = np.array([doc['energy'] for doc in docs]).reshape(-1, 1)\n",
    "\n",
    "# Zip it all together for easier data management\n",
    "data = list(zip(docs, sdts, fingerprints, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved variables 'docs_test, docs_train, docs_val, fingerprints_test, fingerprints_train, fingerprints_val, sdts_test, sdts_train, sdts_val, targets_test, targets_train, targets_val' to file '/global/project/projectdirs/m2755/ktran/sandbox/uncertainty_benchmarking/preprocessing/splits_gasdb.pkl'.]\n",
      "68% train\n",
      "14% validate\n",
      "18% test\n"
     ]
    }
   ],
   "source": [
    "%%cache splits_gasdb.pkl docs_train docs_val docs_test sdts_train sdts_val sdts_test fingerprints_train fingerprints_val fingerprints_test targets_train targets_val targets_test\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gaspy.utils import read_rc\n",
    "\n",
    "\n",
    "# Need this to figure out if the structure is monometallic or not\n",
    "with open(read_rc('gasdb_path') + '/mp_comp_data.pkl', 'rb') as file_handle:\n",
    "    comp_by_mpid = pickle.load(file_handle)\n",
    "\n",
    "# Initialize training set and the \"remainder set\", which we'll eventually split further\n",
    "# into train, validate, and test\n",
    "data_train = []\n",
    "data_remain = []\n",
    "\n",
    "# Allocate the monommetallics to the training set\n",
    "for doc, sdt, fingerprint, target in data:\n",
    "    elements = comp_by_mpid[doc['mpid']]\n",
    "    if len(elements) == 1:\n",
    "        data_train.append((doc, sdt, fingerprint, target))\n",
    "\n",
    "    # Leave everything else to be split normally\n",
    "    else:\n",
    "        data_remain.append((doc, sdt, fingerprint, target))\n",
    "\n",
    "# Figure out all of the adsorbates we'll be looking at.\n",
    "# We will stratify our train/validate/test splits by adsorbate.\n",
    "adsorbates = {doc['adsorbate'] for doc in docs}\n",
    "\n",
    "# Stratify our data by adsorbate\n",
    "data_val = []\n",
    "data_test = []\n",
    "for ads in adsorbates:\n",
    "    _data = [(doc, sdt, fingerprint, target)\n",
    "             for (doc, sdt, fingerprint, target) in data_remain\n",
    "             if doc['adsorbate'] == ads]\n",
    "\n",
    "    # Split out the testing and validation data\n",
    "    data_cv, _data_test = train_test_split(_data, test_size=0.2)\n",
    "    _data_train, _data_val = train_test_split(data_cv, test_size=0.2)\n",
    "\n",
    "    # Concatenate the data in this split with the rest\n",
    "    data_train.extend(_data_train)\n",
    "    data_val.extend(_data_val)\n",
    "    data_test.extend(_data_test)\n",
    "\n",
    "# Shuffle all the datasets because they've been sorted by both adsorbate and monometallics\n",
    "random.shuffle(data_train)\n",
    "random.shuffle(data_val)\n",
    "random.shuffle(data_test)\n",
    "\n",
    "# Parse everything back out explicitly\n",
    "docs_train, sdts_train, fingerprints_train, targets_train = zip(*data_train)\n",
    "docs_val, sdts_val, fingerprints_val, targets_val = zip(*data_val)\n",
    "docs_test, sdts_test, fingerprints_test, targets_test = zip(*data_test)\n",
    "# Turn the tuples into lists\n",
    "docs_train = list(docs_train)\n",
    "sdts_train = list(sdts_train)\n",
    "fingerprints_train = list(fingerprints_train)\n",
    "targets_train = list(targets_train)\n",
    "docs_val = list(docs_val)\n",
    "sdts_val = list(sdts_val)\n",
    "fingerprints_val = list(fingerprints_val)\n",
    "targets_val = list(targets_val)\n",
    "docs_test = list(docs_test)\n",
    "sdts_test = list(sdts_test)\n",
    "fingerprints_test = list(fingerprints_test)\n",
    "targets_test = list(targets_test)\n",
    "\n",
    "# Report the final splits\n",
    "print('%i%% train' % round(len(data_train)/len(data) * 100))\n",
    "print('%i%% validate' % round(len(data_val)/len(data) * 100))\n",
    "print('%i%% test' % round(len(data_test)/len(data) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalysis-Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30420 documents/data points\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('fingerprint/preprocessed_cathub.json', 'rb') as file_handle:\n",
    "    docs = json.load(file_handle)\n",
    "print('%i documents/data points' % len(docs))\n",
    "\n",
    "with open('sdt/cathub/sdts.pkl', 'rb') as file_handle:\n",
    "    sdts = pickle.load(file_handle)\n",
    "\n",
    "with open('fingerprint/fingerprints_cathub.pkl', 'rb') as file_handle:\n",
    "    fingerprints = pickle.load(file_handle)\n",
    "\n",
    "# Targets = adsorption energies\n",
    "targets = np.array([doc['energy'] for doc in docs]).reshape(-1, 1)\n",
    "\n",
    "# Zip it all together for easier data management\n",
    "data = list(zip(docs, sdts, fingerprints, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved variables 'docs_test, docs_train, docs_val, fingerprints_test, fingerprints_train, fingerprints_val, sdts_test, sdts_train, sdts_val, targets_test, targets_train, targets_val' to file '/global/project/projectdirs/m2755/ktran/sandbox/uncertainty_benchmarking/preprocessing/splits_cathub.pkl'.]\n",
      "70% train\n",
      "13% validate\n",
      "16% test\n"
     ]
    }
   ],
   "source": [
    "%%cache splits_cathub.pkl docs_train docs_val docs_test sdts_train sdts_val sdts_test fingerprints_train fingerprints_val fingerprints_test targets_train targets_val targets_test\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Initialize training set and the \"remainder set\", which we'll eventually split further\n",
    "# into train, validate, and test\n",
    "data_train = []\n",
    "data_remain = []\n",
    "\n",
    "# Allocate the monommetallics to the training set\n",
    "for doc, sdt, fingerprint, target in data:\n",
    "    # We assume that the number of elements here is the number of elements\n",
    "    # within the first three neighbor shells of the adsorbate\n",
    "    elements = {element for neighbor in doc['neighborcoord']\n",
    "                for element in neighbor.split(':')[-1].split('-')}\n",
    "    if len(elements) == 1:\n",
    "        data_train.append((doc, sdt, fingerprint, target))\n",
    "\n",
    "    # Leave everything else to be split normally\n",
    "    else:\n",
    "        data_remain.append((doc, sdt, fingerprint, target))\n",
    "\n",
    "# Figure out all of the adsorbates we'll be looking at.\n",
    "# We will stratify our train/validate/test splits by adsorbate.\n",
    "adsorbates = {doc['adsorbate'] for doc in docs}\n",
    "\n",
    "# Stratify our data by adsorbate\n",
    "data_val = []\n",
    "data_test = []\n",
    "for ads in adsorbates:\n",
    "    _data = [(doc, sdt, fingerprint, target)\n",
    "             for (doc, sdt, fingerprint, target) in data_remain\n",
    "             if doc['adsorbate'] == ads]\n",
    "\n",
    "    # Split out the testing and validation data\n",
    "    data_cv, _data_test = train_test_split(_data, test_size=0.2)\n",
    "    _data_train, _data_val = train_test_split(data_cv, test_size=0.2)\n",
    "\n",
    "    # Concatenate the data in this split with the rest\n",
    "    data_train.extend(_data_train)\n",
    "    data_val.extend(_data_val)\n",
    "    data_test.extend(_data_test)\n",
    "\n",
    "# Shuffle all the datasets because they've been sorted by both adsorbate and monometallics\n",
    "random.shuffle(data_train)\n",
    "random.shuffle(data_val)\n",
    "random.shuffle(data_test)\n",
    "\n",
    "# Parse everything back out explicitly\n",
    "docs_train, sdts_train, fingerprints_train, targets_train = zip(*data_train)\n",
    "docs_val, sdts_val, fingerprints_val, targets_val = zip(*data_val)\n",
    "docs_test, sdts_test, fingerprints_test, targets_test = zip(*data_test)\n",
    "# Turn the tuples into lists\n",
    "docs_train = list(docs_train)\n",
    "sdts_train = list(sdts_train)\n",
    "fingerprints_train = list(fingerprints_train)\n",
    "targets_train = list(targets_train)\n",
    "docs_val = list(docs_val)\n",
    "sdts_val = list(sdts_val)\n",
    "fingerprints_val = list(fingerprints_val)\n",
    "targets_val = list(targets_val)\n",
    "docs_test = list(docs_test)\n",
    "sdts_test = list(sdts_test)\n",
    "fingerprints_test = list(fingerprints_test)\n",
    "targets_test = list(targets_test)\n",
    "\n",
    "# Report the final splits\n",
    "print('%i%% train' % (len(data_train)/len(data) * 100))\n",
    "print('%i%% validate' % (len(data_val)/len(data) * 100))\n",
    "print('%i%% test' % (len(data_test)/len(data) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktran",
   "language": "python",
   "name": "ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
